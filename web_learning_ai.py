"""
ğŸŒ ììœ¨ ì •ë³´ ìˆ˜ì§‘ ë° í•™ìŠµ ì˜ì›…ì „ì„¤4 AI
ìŠ¤ìŠ¤ë¡œ ì¸í„°ë„·ì—ì„œ ê²Œì„ ì •ë³´ë¥¼ ì°¾ì•„ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
"""

import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, quote
from datetime import datetime
import sqlite3
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class AutoLearningWebCrawler:
    """ììœ¨ ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ í•™ìŠµ"""
    
    def __init__(self):
        self.session = None
        self.knowledge_db = sqlite3.connect("auto_learned_knowledge.db")
        self.embedding_model = None
        self.search_queries = []
        
        # ì˜ì›…ì „ì„¤4 ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ
        self.base_keywords = [
            "ì˜ì›…ì „ì„¤4", "Legend of Heroes 4", "ê°€ê°€ë¸Œ íŠ¸ë¦´ë¡œì§€",
            "DOSBox", "ë°©í–¥í‚¤", "í‚¤ë³´ë“œ ì¡°ì‘", "ê²Œì„ ì¡°ì‘ë²•",
            "ì „íˆ¬ ì‹œìŠ¤í…œ", "ìºë¦­í„° ì´ë™", "RPG ì¡°ì‘"
        ]
        
        # ìë™ ë°œê²¬í•  í‚¤ì›Œë“œë“¤
        self.discovered_keywords = set()
        
        self.init_knowledge_db()
        self.load_embedding_model()
        
        print("ğŸŒ ììœ¨ í•™ìŠµ ì›¹ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def init_knowledge_db(self):
        """ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        cursor = self.knowledge_db.cursor()
        
        # ì›¹ ì§€ì‹ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS web_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_url TEXT,
                title TEXT,
                content TEXT,
                keywords TEXT,
                relevance_score REAL,
                discovered_at TEXT,
                embedding_vector TEXT,
                knowledge_type TEXT,
                verified BOOLEAN DEFAULT FALSE
            )
        """)
        
        # ë°œê²¬ëœ íŒ¨í„´ í…Œì´ë¸”  
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS discovered_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_name TEXT UNIQUE,
                pattern_description TEXT,
                source_evidence TEXT,
                confidence_score REAL,
                usage_success_rate REAL DEFAULT 0.0,
                times_tested INTEGER DEFAULT 0,
                discovered_at TEXT
            )
        """)
        
        self.knowledge_db.commit()
        print("ğŸ“š ììœ¨ í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
    
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            print("ğŸ”¤ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def autonomous_search(self, current_game_context=""):
        """í˜„ì¬ ê²Œì„ ìƒí™©ì— ë§ëŠ” ììœ¨ ê²€ìƒ‰"""
        
        # í˜„ì¬ ìƒí™© ë¶„ì„í•´ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        search_queries = self.generate_contextual_queries(current_game_context)
        
        print(f"ğŸ” ììœ¨ ê²€ìƒ‰ ì‹œì‘: {len(search_queries)}ê°œ ì¿¼ë¦¬")
        
        async with aiohttp.ClientSession() as session:
            self.session = session
            
            for query in search_queries:
                print(f"   ê²€ìƒ‰: {query}")
                
                try:
                    # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰
                    await self.search_multiple_sources(query)
                    await asyncio.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except Exception as e:
                    print(f"   âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ë° í•™ìŠµ
        self.analyze_and_learn()
    
    def generate_contextual_queries(self, game_context):
        """ê²Œì„ ìƒí™©ì— ë§ëŠ” ê²€ìƒ‰ì–´ ìƒì„±"""
        
        base_queries = [
            "ì˜ì›…ì „ì„¤4 ì¡°ì‘ë²•",
            "ì˜ì›…ì „ì„¤4 í‚¤ë³´ë“œ ì‚¬ìš©ë²•", 
            "Legend of Heroes 4 controls",
            "DOSBox RPG ê²Œì„ ì¡°ì‘",
            "ì˜ì›…ì „ì„¤4 ê³µëµ"
        ]
        
        # í˜„ì¬ ìƒí™©ì— ë”°ë¥¸ ë™ì  ì¿¼ë¦¬ ìƒì„±
        if "ì „íˆ¬" in game_context or "battle" in game_context.lower():
            base_queries.extend([
                "ì˜ì›…ì „ì„¤4 ì „íˆ¬ ì‹œìŠ¤í…œ",
                "ì˜ì›…ì „ì„¤4 ê³µê²© ë°©ë²•",
                "í„´ì œ RPG ì „íˆ¬ ì¡°ì‘ë²•"
            ])
        
        if "ì´ë™" in game_context or "movement" in game_context.lower():
            base_queries.extend([
                "ì˜ì›…ì „ì„¤4 ìºë¦­í„° ì´ë™",
                "ë°©í–¥í‚¤ ì‚¬ìš©ë²•",
                "RPG ê²Œì„ íƒí—˜ ë°©ë²•"
            ])
        
        if "ë©”ë‰´" in game_context or "UI" in game_context:
            base_queries.extend([
                "ì˜ì›…ì „ì„¤4 ë©”ë‰´ ì‚¬ìš©ë²•",
                "ê²Œì„ ì¸ë²¤í† ë¦¬ ì¡°ì‘",
                "RPG ìƒíƒœì°½ ë³´ëŠ”ë²•"
            ])
        
        return base_queries[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    async def search_multiple_sources(self, query):
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰"""
        
        sources = [
            self.search_namu_wiki,
            self.search_google_snippets,
            self.search_game_community,
        ]
        
        tasks = []
        for source_func in sources:
            task = asyncio.create_task(source_func(query))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì €ì¥
        for result in results:
            if isinstance(result, Exception):
                continue
            if result:
                self.store_web_knowledge(result, query)
    
    async def search_namu_wiki(self, query):
        """ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ê²€ìƒ‰"""
        try:
            search_url = f"https://namu.wiki/w/{quote(query)}"
            
            async with self.session.get(search_url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
                    content_div = soup.find('div', class_='wiki-content')
                    if content_div:
                        text = content_div.get_text(strip=True)
                        
                        return {
                            'source': 'ë‚˜ë¬´ìœ„í‚¤',
                            'url': search_url,
                            'title': query,
                            'content': text[:2000],  # ì²˜ìŒ 2000ìë§Œ
                            'type': 'wiki'
                        }
        except Exception as e:
            print(f"ë‚˜ë¬´ìœ„í‚¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def search_google_snippets(self, query):
        """êµ¬ê¸€ ê²€ìƒ‰ ìŠ¤ë‹ˆí« ìˆ˜ì§‘"""
        try:
            # DuckDuckGo API ì‚¬ìš© (êµ¬ê¸€ ëŒ€ì‹ )
            search_url = f"https://api.duckduckgo.com/?q={quote(query)}&format=json&no_html=1&skip_disambig=1"
            
            async with self.session.get(search_url, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Abstractë‚˜ Answerì—ì„œ ì •ë³´ ì¶”ì¶œ
                    content = ""
                    if data.get('Abstract'):
                        content += data['Abstract']
                    if data.get('Answer'):
                        content += " " + data['Answer']
                    
                    if content:
                        return {
                            'source': 'DuckDuckGo',
                            'url': search_url,
                            'title': query,
                            'content': content,
                            'type': 'search_snippet'
                        }
        except Exception as e:
            print(f"ê²€ìƒ‰ ìŠ¤ë‹ˆí« ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def search_game_community(self, query):
        """ê²Œì„ ì»¤ë®¤ë‹ˆí‹° ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” ë£¨ë¦¬ì›¹, ë””ì‹œì¸ì‚¬ì´ë“œ ë“±ì—ì„œ ê²€ìƒ‰í•˜ì§€ë§Œ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼ ë°˜í™˜
        
        game_tips = {
            "ë°©í–¥í‚¤": "ìƒí•˜ì¢Œìš° ë°©í–¥í‚¤ë¡œ ìºë¦­í„° ì´ë™. Enterí‚¤ë¡œ í™•ì¸, ESCí‚¤ë¡œ ì·¨ì†Œ",
            "ì „íˆ¬": "ì „íˆ¬ ì‹œ ìˆ«ìí‚¤ë¡œ ê³µê²© ì„ íƒ, ë°©í–¥í‚¤ë¡œ ëŒ€ìƒ ì„ íƒ",
            "ë©”ë‰´": "Altí‚¤ë¡œ ë©”ë‰´ í˜¸ì¶œ, Tabí‚¤ë¡œ ìƒíƒœì°½ í™•ì¸",
            "ì´ë™": "í•„ë“œì—ì„œ ë°©í–¥í‚¤ë¡œ 8ë°©í–¥ ì´ë™ ê°€ëŠ¥",
            "ì¡°ì‘": "ê¸°ë³¸ì ìœ¼ë¡œ í‚¤ë³´ë“œë§Œ ì‚¬ìš©í•˜ë©°, ë§ˆìš°ìŠ¤ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ"
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ íŒ ì°¾ê¸°
        relevant_tips = []
        for keyword, tip in game_tips.items():
            if keyword in query or any(k in query for k in keyword):
                relevant_tips.append(tip)
        
        if relevant_tips:
            return {
                'source': 'ê²Œì„ ì»¤ë®¤ë‹ˆí‹°',
                'url': 'simulated',
                'title': f'{query} ê´€ë ¨ íŒ',
                'content': " ".join(relevant_tips),
                'type': 'community_tip'
            }
        
        return None
    
    def store_web_knowledge(self, knowledge_data, original_query):
        """ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì§€ì‹ ì €ì¥"""
        if not knowledge_data:
            return
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_score = self.calculate_relevance(knowledge_data['content'], original_query)
        
        # ì„ë² ë”© ë²¡í„° ìƒì„±
        embedding_vector = None
        if self.embedding_model:
            embedding = self.embedding_model.encode(knowledge_data['content'])
            embedding_vector = json.dumps(embedding.tolist())
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(knowledge_data['content'])
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        cursor = self.knowledge_db.cursor()
        cursor.execute("""
            INSERT INTO web_knowledge 
            (source_url, title, content, keywords, relevance_score, 
             discovered_at, embedding_vector, knowledge_type)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            knowledge_data['url'],
            knowledge_data['title'], 
            knowledge_data['content'],
            json.dumps(keywords),
            relevance_score,
            datetime.now().isoformat(),
            embedding_vector,
            knowledge_data['type']
        ))
        
        self.knowledge_db.commit()
        print(f"ğŸ’¾ ì§€ì‹ ì €ì¥: {knowledge_data['title']} (ê´€ë ¨ì„±: {relevance_score:.2f})")
    
    def calculate_relevance(self, content, query):
        """ë‚´ìš©ê³¼ ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê°„ë‹¨í•œ ê´€ë ¨ì„± ê³„ì‚°
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())
        
        # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨
        intersection = query_words.intersection(content_words)
        union = query_words.union(content_words)
        
        jaccard_similarity = len(intersection) / len(union) if union else 0
        
        # ê²Œì„ ê´€ë ¨ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
        game_keywords = ['ì˜ì›…ì „ì„¤', 'ë°©í–¥í‚¤', 'í‚¤ë³´ë“œ', 'ì¡°ì‘', 'rpg', 'ê²Œì„']
        bonus = sum(1 for keyword in game_keywords if keyword in content.lower()) * 0.1
        
        return min(1.0, jaccard_similarity + bonus)
    
    def extract_keywords(self, content):
        """ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ê²Œì„ ì¡°ì‘ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        control_patterns = [
            r'ë°©í–¥í‚¤',
            r'Enter|ì—”í„°',
            r'ESC|Escape|ì·¨ì†Œ',
            r'Alt|ì•ŒíŠ¸',
            r'Tab|íƒ­',
            r'Shift|ì‹œí”„íŠ¸',
            r'Ctrl|ì»¨íŠ¸ë¡¤',
            r'ìŠ¤í˜ì´ìŠ¤|Space',
            r'ìˆ«ìí‚¤',
            r'í‚¤ë³´ë“œ',
            r'ë§ˆìš°ìŠ¤'
        ]
        
        # ê²Œì„ ìš©ì–´ íŒ¨í„´
        game_patterns = [
            r'ì „íˆ¬|ë°°í‹€',
            r'ì´ë™|ì›€ì§ì„',
            r'ê³µê²©|ì–´íƒ',
            r'ë°©ì–´|ë””íœìŠ¤', 
            r'ë©”ë‰´|ì¸ë²¤í† ë¦¬',
            r'ìƒíƒœì°½|ìŠ¤í…Œì´í„°ìŠ¤',
            r'ìºë¦­í„°|ì£¼ì¸ê³µ',
            r'ì |ëª¬ìŠ¤í„°|enemy',
            r'ë ˆë²¨|ê²½í—˜ì¹˜',
            r'ì•„ì´í…œ|ì¥ë¹„'
        ]
        
        found_keywords = []
        
        # íŒ¨í„´ ë§¤ì¹­
        for patterns in [control_patterns, game_patterns]:
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                found_keywords.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_keywords = list(set(found_keywords))
        
        return unique_keywords[:20]  # ìµœëŒ€ 20ê°œ
    
    def analyze_and_learn(self):
        """ìˆ˜ì§‘ëœ ì •ë³´ ë¶„ì„ ë° íŒ¨í„´ í•™ìŠµ"""
        
        print("ğŸ§  ìˆ˜ì§‘ëœ ì •ë³´ ë¶„ì„ ì¤‘...")
        
        cursor = self.knowledge_db.cursor()
        cursor.execute("""
            SELECT content, keywords, relevance_score 
            FROM web_knowledge 
            WHERE relevance_score > 0.3 
            ORDER BY discovered_at DESC 
            LIMIT 50
        """)
        
        recent_knowledge = cursor.fetchall()
        
        if not recent_knowledge:
            print("âŒ ë¶„ì„í•  ì§€ì‹ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return
        
        # í‚¤ë³´ë“œ ì¡°ì‘ íŒ¨í„´ ì¶”ì¶œ
        control_patterns = self.extract_control_patterns(recent_knowledge)
        
        # ê²Œì„ í”Œë ˆì´ íŒ ì¶”ì¶œ  
        gameplay_tips = self.extract_gameplay_tips(recent_knowledge)
        
        # í•™ìŠµëœ íŒ¨í„´ ì €ì¥
        for pattern_name, pattern_data in control_patterns.items():
            self.store_discovered_pattern(pattern_name, pattern_data, "keyboard_control")
        
        for tip_name, tip_data in gameplay_tips.items():
            self.store_discovered_pattern(tip_name, tip_data, "gameplay_strategy")
        
        print(f"ğŸ’¡ ë°œê²¬ëœ íŒ¨í„´: {len(control_patterns) + len(gameplay_tips)}ê°œ")
    
    def extract_control_patterns(self, knowledge_list):
        """í‚¤ë³´ë“œ ì¡°ì‘ íŒ¨í„´ ì¶”ì¶œ"""
        
        patterns = {}
        
        for content, keywords_json, relevance in knowledge_list:
            try:
                keywords = json.loads(keywords_json)
            except:
                keywords = []
            
            # ë°©í–¥í‚¤ íŒ¨í„´
            if any('ë°©í–¥í‚¤' in str(k) for k in keywords):
                patterns['direction_keys'] = {
                    'description': 'ë°©í–¥í‚¤ë¡œ ìºë¦­í„° ì´ë™ ì œì–´',
                    'keys': ['UP', 'DOWN', 'LEFT', 'RIGHT'],
                    'confidence': min(1.0, relevance * 1.2)
                }
            
            # í™•ì¸/ì·¨ì†Œ íŒ¨í„´
            if any(k in str(keywords).lower() for k in ['enter', 'ì—”í„°', 'esc', 'escape']):
                patterns['confirm_cancel'] = {
                    'description': 'Enterë¡œ í™•ì¸, ESCë¡œ ì·¨ì†Œ',
                    'keys': ['ENTER', 'ESCAPE'], 
                    'confidence': min(1.0, relevance * 1.1)
                }
            
            # ë©”ë‰´ íŒ¨í„´
            if any(k in str(keywords).lower() for k in ['alt', 'ì•ŒíŠ¸', 'tab', 'íƒ­']):
                patterns['menu_access'] = {
                    'description': 'Altë¡œ ë©”ë‰´, Tabìœ¼ë¡œ ìƒíƒœì°½',
                    'keys': ['ALT', 'TAB'],
                    'confidence': min(1.0, relevance)
                }
        
        return patterns
    
    def extract_gameplay_tips(self, knowledge_list):
        """ê²Œì„í”Œë ˆì´ íŒ ì¶”ì¶œ"""
        
        tips = {}
        
        for content, keywords_json, relevance in knowledge_list:
            content_lower = content.lower()
            
            # ì „íˆ¬ ê´€ë ¨ íŒ
            if 'ì „íˆ¬' in content_lower or 'battle' in content_lower:
                tips['battle_strategy'] = {
                    'description': 'ì „íˆ¬ ì‹œ ì „ëµì  í–‰ë™ í•„ìš”',
                    'evidence': content[:200],
                    'confidence': relevance
                }
            
            # íƒí—˜ ê´€ë ¨ íŒ  
            if 'íƒí—˜' in content_lower or 'ì´ë™' in content_lower:
                tips['exploration_strategy'] = {
                    'description': 'ì²´ê³„ì  íƒí—˜ìœ¼ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€',
                    'evidence': content[:200], 
                    'confidence': relevance
                }
        
        return tips
    
    def store_discovered_pattern(self, pattern_name, pattern_data, pattern_type):
        """ë°œê²¬ëœ íŒ¨í„´ ì €ì¥"""
        
        cursor = self.knowledge_db.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO discovered_patterns
            (pattern_name, pattern_description, source_evidence, 
             confidence_score, discovered_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            f"{pattern_type}_{pattern_name}",
            pattern_data['description'],
            json.dumps(pattern_data),
            pattern_data['confidence'],
            datetime.now().isoformat()
        ))
        
        self.knowledge_db.commit()
    
    def get_relevant_knowledge(self, current_situation):
        """í˜„ì¬ ìƒí™©ì— ë§ëŠ” í•™ìŠµëœ ì§€ì‹ ê²€ìƒ‰"""
        
        cursor = self.knowledge_db.cursor()
        cursor.execute("""
            SELECT pattern_name, pattern_description, source_evidence, confidence_score
            FROM discovered_patterns
            WHERE confidence_score > 0.5
            ORDER BY confidence_score DESC
        """)
        
        patterns = cursor.fetchall()
        
        # í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨ì„± ìˆëŠ” íŒ¨í„´ í•„í„°ë§
        relevant_patterns = []
        situation_lower = current_situation.lower()
        
        for name, description, evidence, confidence in patterns:
            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ì„± íŒë‹¨
            if any(keyword in situation_lower for keyword in 
                   ['ì´ë™', 'ì „íˆ¬', 'ë©”ë‰´', 'move', 'battle', 'menu']):
                
                if any(keyword in name.lower() for keyword in
                       ['direction', 'confirm', 'menu', 'battle', 'exploration']):
                    
                    relevant_patterns.append({
                        'name': name,
                        'description': description,
                        'confidence': confidence,
                        'evidence': evidence
                    })
        
        return relevant_patterns[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def get_learning_summary(self):
        """í•™ìŠµ í˜„í™© ìš”ì•½"""
        
        cursor = self.knowledge_db.cursor()
        
        # ìˆ˜ì§‘ëœ ì§€ì‹ í†µê³„
        cursor.execute("SELECT COUNT(*) FROM web_knowledge")
        total_knowledge = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM discovered_patterns") 
        total_patterns = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(confidence_score) FROM discovered_patterns")
        avg_confidence = cursor.fetchone()[0] or 0
        
        return {
            'total_web_knowledge': total_knowledge,
            'discovered_patterns': total_patterns,
            'average_confidence': avg_confidence,
            'last_learning': datetime.now().isoformat()
        }


class WebEnhancedGameAI:
    """ì›¹ í•™ìŠµ ê°•í™” ê²Œì„ AI"""
    
    def __init__(self):
        from isolated_seeker import IsolatedDOSBoxSeeker
        
        self.base_seeker = IsolatedDOSBoxSeeker()
        self.web_crawler = AutoLearningWebCrawler()
        
        # LLM ì„¤ì •
        self.llm_endpoint = "http://localhost:11434/api/generate"
        self.model = "qwen2.5-coder:7b"
        
        # í•™ìŠµ ìƒíƒœ
        self.learning_phase = "initial"  # initial -> informed -> expert
        self.web_learning_completed = False
        
        print("ğŸŒ ì›¹ í•™ìŠµ ê°•í™” ê²Œì„ AI ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initial_web_learning(self):
        """ì´ˆê¸° ì›¹ í•™ìŠµ ë‹¨ê³„"""
        
        print("ğŸ” ê²Œì„ì— ëŒ€í•œ ì´ˆê¸° ì›¹ í•™ìŠµ ì‹œì‘...")
        
        # ê¸°ë³¸ ê²Œì„ ì •ë³´ ìˆ˜ì§‘
        await self.web_crawler.autonomous_search("ì˜ì›…ì „ì„¤4 ê¸°ë³¸ ì¡°ì‘ë²•")
        
        # ìˆ˜ì§‘ëœ ì§€ì‹ ìš”ì•½
        summary = self.web_crawler.get_learning_summary()
        print(f"ğŸ“š ì´ˆê¸° í•™ìŠµ ì™„ë£Œ: {summary['total_web_knowledge']}ê°œ ì§€ì‹, {summary['discovered_patterns']}ê°œ íŒ¨í„´")
        
        self.web_learning_completed = True
        self.learning_phase = "informed"
    
    def get_web_informed_decision(self, current_situation):
        """ì›¹ì—ì„œ í•™ìŠµí•œ ì§€ì‹ ê¸°ë°˜ ê²°ì •"""
        
        # ê´€ë ¨ íŒ¨í„´ ê²€ìƒ‰
        relevant_knowledge = self.web_crawler.get_relevant_knowledge(current_situation)
        
        if not relevant_knowledge:
            return self.fallback_decision()
        
        # LLMì—ê²Œ ì›¹ ì§€ì‹ê³¼ í•¨ê»˜ ë¶„ì„ ìš”ì²­
        web_informed_prompt = f"""
ë‹¹ì‹ ì€ ì›¹ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê²Œì„ì„ í”Œë ˆì´í•˜ëŠ” AIì…ë‹ˆë‹¤.

í˜„ì¬ ìƒí™©: {current_situation}

ì›¹ì—ì„œ í•™ìŠµí•œ ê´€ë ¨ ì§€ì‹:
{self.format_web_knowledge(relevant_knowledge)}

ì´ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ì„¸ìš” (0-6):
0: ì™¼ìª½ ì´ë™
1: ì˜¤ë¥¸ìª½ ì´ë™  
2: ìœ„ìª½ ì´ë™
3: ì•„ë˜ìª½ ì´ë™
4: í™•ì¸/ê³µê²© (Enter)
5: ì·¨ì†Œ/ë©”ë‰´ (ESC)
6: ëŒ€ê¸°

JSON í˜•íƒœë¡œ:
{{
    "action": 1,
    "reasoning": "ì›¹ì—ì„œ í•™ìŠµí•œ ì§€ì‹ ê¸°ë°˜ íŒë‹¨",
    "web_knowledge_used": "ì‚¬ìš©ëœ ì›¹ ì§€ì‹ ì„¤ëª…",
    "confidence": 0.8
}}
"""
        
        try:
            response = self.call_llm(web_informed_prompt)
            decision = self.parse_llm_json(response)
            
            if decision and 'action' in decision:
                print(f"ğŸŒ ì›¹ ì§€ì‹ ê¸°ë°˜ ê²°ì •: {decision['reasoning']}")
                return decision
            
        except Exception as e:
            print(f"âš ï¸ ì›¹ ì§€ì‹ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return self.fallback_decision()
    
    def format_web_knowledge(self, knowledge_list):
        """ì›¹ ì§€ì‹ì„ LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        
        if not knowledge_list:
            return "ê´€ë ¨ëœ ì›¹ ì§€ì‹ì´ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for i, knowledge in enumerate(knowledge_list, 1):
            formatted.append(f"""
ì§€ì‹ {i} (ì‹ ë¢°ë„: {knowledge['confidence']:.2f}):
- íŒ¨í„´: {knowledge['name']}
- ì„¤ëª…: {knowledge['description']}
""")
        
        return "\n".join(formatted)
    
    async def adaptive_web_learning(self, current_context):
        """ìƒí™©ì— ë§ëŠ” ì ì‘ì  ì›¹ í•™ìŠµ"""
        
        # í˜„ì¬ ìƒí™©ì—ì„œ ë¶€ì¡±í•œ ì§€ì‹ì´ ìˆìœ¼ë©´ ì¶”ê°€ í•™ìŠµ
        relevant_knowledge = self.web_crawler.get_relevant_knowledge(current_context)
        
        if len(relevant_knowledge) < 2:  # ê´€ë ¨ ì§€ì‹ì´ ë¶€ì¡±í•˜ë©´
            print(f"ğŸ” ì¶”ê°€ ì›¹ í•™ìŠµ í•„ìš”: {current_context}")
            await self.web_crawler.autonomous_search(current_context)
    
    def call_llm(self, prompt, timeout=10):
        """LLM í˜¸ì¶œ"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.3, "top_p": 0.9}
        }
        
        response = requests.post(self.llm_endpoint, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            raise Exception(f"LLM API ì˜¤ë¥˜: {response.status_code}")
    
    def parse_llm_json(self, text):
        """LLM ì‘ë‹µ JSON íŒŒì‹±"""
        try:
            start = text.find("{")
            end = text.rfind("}") + 1
            if start != -1 and end > start:
                json_text = text[start:end]
                return json.loads(json_text)
        except:
            pass
        return {}
    
    def fallback_decision(self):
        """í´ë°± ê²°ì •"""
        return {
            "action": np.random.randint(0, 7),
            "reasoning": "ì›¹ ì§€ì‹ ë¶€ì¡±ìœ¼ë¡œ ëœë¤ íƒí—˜",
            "confidence": 0.3
        }
    
    def execute_action(self, action_id):
        """í–‰ë™ ì‹¤í–‰"""
        vk_keys = {
            0: self.base_seeker.VK_LEFT,
            1: self.base_seeker.VK_RIGHT,
            2: self.base_seeker.VK_UP,
            3: self.base_seeker.VK_DOWN,
            4: self.base_seeker.VK_RETURN,
            5: self.base_seeker.VK_ESCAPE,
            6: None
        }
        
        vk_code = vk_keys.get(action_id)
        if vk_code is not None:
            return self.base_seeker.send_key_message(vk_code)
        return True
    
    async def web_enhanced_play(self, max_iterations=100):
        """ì›¹ í•™ìŠµ ê°•í™” ììœ¨ í”Œë ˆì´"""
        
        print("ğŸŒ ì›¹ í•™ìŠµ ê°•í™” ììœ¨ í”Œë ˆì´ ì‹œì‘!")
        
        if not self.base_seeker.find_dosbox_window():
            print("âŒ DOSBox ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì´ˆê¸° ì›¹ í•™ìŠµ
        if not self.web_learning_completed:
            await self.initial_web_learning()
        
        iteration = 0
        
        while iteration < max_iterations:
            try:
                print(f"\n--- ì›¹ ê°•í™” í”Œë ˆì´ #{iteration + 1} ---")
                
                # í˜„ì¬ í™”ë©´ ìº¡ì²˜ ë° ë¶„ì„
                current_screen = self.base_seeker.capture_dosbox_window()
                if current_screen is None:
                    continue
                
                # ê²Œì„ ìƒí™© ë¶„ì„
                is_battle = self.base_seeker.is_battle_screen(current_screen)
                current_situation = "ì „íˆ¬ ìƒí™©" if is_battle else "í•„ë“œ íƒí—˜"
                
                print(f"ğŸ® í˜„ì¬ ìƒí™©: {current_situation}")
                
                # ìƒí™©ì— ë§ëŠ” ì¶”ê°€ ì›¹ í•™ìŠµ (í•„ìš”ì‹œ)
                await self.adaptive_web_learning(current_situation)
                
                # ì›¹ ì§€ì‹ ê¸°ë°˜ ê²°ì •
                decision = self.get_web_informed_decision(current_situation)
                
                action = decision["action"]
                reasoning = decision.get("reasoning", "")
                web_knowledge = decision.get("web_knowledge_used", "")
                
                print(f"ğŸ§  ì„ íƒí•œ í–‰ë™: {action} - {reasoning}")
                if web_knowledge:
                    print(f"ğŸ“š í™œìš©ëœ ì›¹ ì§€ì‹: {web_knowledge}")
                
                # í–‰ë™ ì‹¤í–‰
                self.execute_action(action)
                
                # ê²°ê³¼ ê´€ì°°
                time.sleep(1.5)
                
                iteration += 1
                
                # ì£¼ê¸°ì  í•™ìŠµ ìš”ì•½
                if iteration % 10 == 0:
                    summary = self.web_crawler.get_learning_summary()
                    print(f"\nğŸ“Š í•™ìŠµ í˜„í™© ({iteration}íšŒ í”Œë ˆì´)")
                    print(f"   ì›¹ ì§€ì‹: {summary['total_web_knowledge']}ê°œ")
                    print(f"   ë°œê²¬ëœ íŒ¨í„´: {summary['discovered_patterns']}ê°œ")
                    print(f"   í‰ê·  ì‹ ë¢°ë„: {summary['average_confidence']:.2f}")
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸ í”Œë ˆì´ ì¤‘ë‹¨")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                iteration += 1
        
        print("\nğŸ‰ ì›¹ í•™ìŠµ ê°•í™” í”Œë ˆì´ ì™„ë£Œ!")
        self.print_final_web_report()
    
    def print_final_web_report(self):
        """ìµœì¢… ì›¹ í•™ìŠµ ë³´ê³ ì„œ"""
        summary = self.web_crawler.get_learning_summary()
        
        print("\n" + "="*60)
        print("ğŸŒ ì›¹ í•™ìŠµ ê°•í™” AI ìµœì¢… ë³´ê³ ì„œ")
        print("="*60)
        
        print(f"ğŸ“š í•™ìŠµ ì„±ê³¼:")
        print(f"   ìˆ˜ì§‘ëœ ì›¹ ì§€ì‹: {summary['total_web_knowledge']}ê°œ")
        print(f"   ë°œê²¬í•œ íŒ¨í„´: {summary['discovered_patterns']}ê°œ")
        print(f"   íŒ¨í„´ ì‹ ë¢°ë„: {summary['average_confidence']:.2f}")
        
        print(f"\nğŸ¯ ììœ¨ í•™ìŠµ íŠ¹ì§•:")
        print(f"   - ì¸í„°ë„·ì—ì„œ ìë™ìœ¼ë¡œ ê²Œì„ ì •ë³´ ìˆ˜ì§‘")
        print(f"   - ìƒí™©ì— ë§ëŠ” ì ì‘ì  ì¶”ê°€ í•™ìŠµ")
        print(f"   - ì›¹ ì§€ì‹ì„ ê²Œì„ í”Œë ˆì´ì— ì‹¤ì‹œê°„ ì ìš©")
        print(f"   - ì§€ì†ì ì¸ íŒ¨í„´ ë°œê²¬ ë° ê°œì„ ")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸŒ ì›¹ í•™ìŠµ ê°•í™” ì˜ì›…ì „ì„¤4 AI")
    print("ìŠ¤ìŠ¤ë¡œ ì¸í„°ë„·ì—ì„œ ê²Œì„ ì •ë³´ë¥¼ ì°¾ì•„ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ")
    print()
    
    print("ğŸš€ íŠ¹ì§•:")
    print("- ììœ¨ì  ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ê²Œì„ ì •ë³´ ìˆ˜ì§‘")
    print("- í‚¤ë³´ë“œ ì¡°ì‘ë²•, ê²Œì„ íŒ ìë™ í•™ìŠµ")
    print("- ìƒí™©ë³„ ì ì‘ì  ì¶”ê°€ í•™ìŠµ")
    print("- ì›¹ ì§€ì‹ì„ ì‹¤ì‹œê°„ ê²Œì„ í”Œë ˆì´ì— í™œìš©")
    print()
    
    try:
        iterations = input("í•™ìŠµ í”Œë ˆì´ íšŸìˆ˜ (ê¸°ë³¸ê°’ 50): ").strip()
        max_iterations = int(iterations) if iterations else 50
        
        print(f"\nğŸŒ {max_iterations}íšŒ ì›¹ í•™ìŠµ ê°•í™” í”Œë ˆì´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("ğŸ’¡ ì²˜ìŒì—ëŠ” ì›¹ì—ì„œ ê²Œì„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (1-2ë¶„ ì†Œìš”)")
        print("\nì‹œì‘í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        input()
        
        # ì›¹ ê°•í™” AI ì‹¤í–‰
        ai = WebEnhancedGameAI()
        await ai.web_enhanced_play(max_iterations=max_iterations)
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì›¹ í•™ìŠµ AI ê°œë°œ ì™„ë£Œ!")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())