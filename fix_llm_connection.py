"""
ğŸ”§ LLM ì—°ê²° ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²° ë„êµ¬
Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ë° ìë™ ë³µêµ¬
"""

import requests
import subprocess
import time
import json
import os
import psutil

class LLMConnectionFixer:
    """LLM ì—°ê²° ë¬¸ì œ ìë™ í•´ê²°"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.required_models = [
            "qwen2:0.5b",
            "llama3.2:1b", 
            "qwen2.5-coder:7b"
        ]
        
    def diagnose_connection(self):
        """ì—°ê²° ë¬¸ì œ ì§„ë‹¨"""
        print("ğŸ” LLM ì—°ê²° ìƒíƒœ ì§„ë‹¨ ì¤‘...")
        
        issues = []
        
        # 1. Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
        print("\n1ï¸âƒ£ Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸...")
        ollama_running = self.is_ollama_process_running()
        
        if ollama_running:
            print("âœ… Ollama í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘")
        else:
            print("âŒ Ollama í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            issues.append("ollama_process")
        
        # 2. ì„œë²„ ì‘ë‹µ í™•ì¸
        print("\n2ï¸âƒ£ ì„œë²„ ì‘ë‹µ í™•ì¸...")
        server_responsive = self.test_server_response()
        
        if server_responsive:
            print("âœ… Ollama ì„œë²„ ì‘ë‹µ ì •ìƒ")
        else:
            print("âŒ Ollama ì„œë²„ ì‘ë‹µ ì—†ìŒ")
            issues.append("server_response")
        
        # 3. ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
        print("\n3ï¸âƒ£ ëª¨ë¸ ì„¤ì¹˜ ìƒíƒœ í™•ì¸...")
        installed_models = self.get_installed_models()
        
        if installed_models:
            print(f"âœ… ì„¤ì¹˜ëœ ëª¨ë¸: {len(installed_models)}ê°œ")
            for model in installed_models[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   - {model}")
        else:
            print("âŒ ì„¤ì¹˜ëœ ëª¨ë¸ ì—†ìŒ")
            issues.append("no_models")
        
        # 4. í•„ìˆ˜ ëª¨ë¸ í™•ì¸
        print("\n4ï¸âƒ£ í•„ìˆ˜ ëª¨ë¸ í™•ì¸...")
        missing_models = []
        
        for model in self.required_models:
            if model in installed_models:
                print(f"âœ… {model} ì„¤ì¹˜ë¨")
            else:
                print(f"âŒ {model} ëˆ„ë½")
                missing_models.append(model)
        
        if missing_models:
            issues.append(("missing_models", missing_models))
        
        # 5. API í…ŒìŠ¤íŠ¸
        print("\n5ï¸âƒ£ API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        api_working = self.test_api_functionality()
        
        if api_working:
            print("âœ… API ê¸°ëŠ¥ ì •ìƒ")
        else:
            print("âŒ API ê¸°ëŠ¥ ì˜¤ë¥˜")
            issues.append("api_error")
        
        return issues
    
    def is_ollama_process_running(self):
        """Ollama í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ í™•ì¸"""
        try:
            for proc in psutil.process_iter(['pid', 'name']):
                if 'ollama' in proc.info['name'].lower():
                    return True
            return False
        except:
            return False
    
    def test_server_response(self):
        """ì„œë²„ ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            return response.status_code == 200
        except:
            return False
    
    def get_installed_models(self):
        """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                return [model["name"] for model in models_data.get("models", [])]
            return []
        except:
            return []
    
    def test_api_functionality(self):
        """API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            installed_models = self.get_installed_models()
            if not installed_models:
                return False
            
            # ì²« ë²ˆì§¸ ëª¨ë¸ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_model = installed_models[0]
            
            payload = {
                "model": test_model,
                "prompt": "Say OK",
                "stream": False,
                "options": {"num_predict": 5}
            }
            
            response = requests.post(f"{self.ollama_url}/api/generate", 
                                   json=payload, timeout=10)
            
            return response.status_code == 200
            
        except:
            return False
    
    def fix_issues(self, issues):
        """ë¬¸ì œ ìë™ í•´ê²°"""
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ì‹œì‘...")
        
        for issue in issues:
            if issue == "ollama_process":
                self.start_ollama_server()
            
            elif issue == "server_response":
                self.restart_ollama_server()
            
            elif issue == "no_models":
                self.install_basic_models()
            
            elif isinstance(issue, tuple) and issue[0] == "missing_models":
                missing_models = issue[1]
                self.install_missing_models(missing_models)
            
            elif issue == "api_error":
                self.fix_api_issues()
    
    def start_ollama_server(self):
        """Ollama ì„œë²„ ì‹œì‘"""
        print("ğŸš€ Ollama ì„œë²„ ì‹œì‘ ì¤‘...")
        
        try:
            # Windowsì—ì„œ ollama serve ì‹¤í–‰
            subprocess.Popen(["ollama", "serve"], 
                           creationflags=subprocess.CREATE_NO_WINDOW)
            
            print("â³ ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘...")
            time.sleep(5)
            
            # ì‹œì‘ í™•ì¸
            if self.test_server_response():
                print("âœ… Ollama ì„œë²„ ì‹œì‘ ì™„ë£Œ!")
                return True
            else:
                print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì„œë²„ ì‹œì‘ ì˜¤ë¥˜: {e}")
            return False
    
    def restart_ollama_server(self):
        """Ollama ì„œë²„ ì¬ì‹œì‘"""
        print("ğŸ”„ Ollama ì„œë²„ ì¬ì‹œì‘ ì¤‘...")
        
        # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        self.kill_ollama_processes()
        time.sleep(2)
        
        # ì„œë²„ ì¬ì‹œì‘
        return self.start_ollama_server()
    
    def kill_ollama_processes(self):
        """Ollama í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"""
        try:
            for proc in psutil.process_iter(['pid', 'name']):
                if 'ollama' in proc.info['name'].lower():
                    proc.terminate()
            time.sleep(1)
        except:
            pass
    
    def install_basic_models(self):
        """ê¸°ë³¸ ëª¨ë¸ ì„¤ì¹˜"""
        print("ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ì„¤ì¹˜ ì¤‘...")
        
        basic_models = ["llama3.2:1b", "qwen2:0.5b"]
        
        for model in basic_models:
            self.install_model(model)
    
    def install_missing_models(self, missing_models):
        """ëˆ„ë½ëœ ëª¨ë¸ ì„¤ì¹˜"""
        print(f"ğŸ“¥ ëˆ„ë½ëœ ëª¨ë¸ ì„¤ì¹˜: {missing_models}")
        
        for model in missing_models:
            self.install_model(model)
    
    def install_model(self, model_name):
        """ê°œë³„ ëª¨ë¸ ì„¤ì¹˜"""
        print(f"ğŸ“¦ {model_name} ì„¤ì¹˜ ì¤‘...")
        
        try:
            result = subprocess.run(["ollama", "pull", model_name], 
                                  capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                print(f"âœ… {model_name} ì„¤ì¹˜ ì™„ë£Œ")
                return True
            else:
                print(f"âŒ {model_name} ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° {model_name} ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"âŒ {model_name} ì„¤ì¹˜ ì˜¤ë¥˜: {e}")
            return False
    
    def fix_api_issues(self):
        """API ë¬¸ì œ í•´ê²°"""
        print("ğŸ”§ API ë¬¸ì œ í•´ê²° ì¤‘...")
        
        # ì„œë²„ ì¬ì‹œì‘ìœ¼ë¡œ ëŒ€ë¶€ë¶„ í•´ê²°ë¨
        return self.restart_ollama_server()
    
    def run_comprehensive_fix(self):
        """ì¢…í•© ë¬¸ì œ í•´ê²°"""
        print("ğŸ› ï¸ LLM ì—°ê²° ì¢…í•© ì§„ë‹¨ ë° í•´ê²°")
        print("="*50)
        
        # 1. ì§„ë‹¨
        issues = self.diagnose_connection()
        
        if not issues:
            print("\nğŸ‰ ëª¨ë“  ê²ƒì´ ì •ìƒì…ë‹ˆë‹¤!")
            return True
        
        print(f"\nâŒ ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ")
        
        # 2. í•´ê²°
        self.fix_issues(issues)
        
        # 3. ì¬ê²€ì¦
        print("\nğŸ” í•´ê²° í›„ ì¬ê²€ì¦...")
        time.sleep(3)
        
        remaining_issues = self.diagnose_connection()
        
        if not remaining_issues:
            print("\nğŸ‰ ëª¨ë“  ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
            self.test_final_connection()
            return True
        else:
            print(f"\nâš ï¸ ë‚¨ì€ ë¬¸ì œ: {len(remaining_issues)}ê°œ")
            print("ìˆ˜ë™ í•´ê²°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return False
    
    def test_final_connection(self):
        """ìµœì¢… ì—°ê²° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ìµœì¢… ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        try:
            installed_models = self.get_installed_models()
            if not installed_models:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ ì„ íƒ
            test_model = None
            for preferred in ["qwen2:0.5b", "llama3.2:1b"]:
                if preferred in installed_models:
                    test_model = preferred
                    break
            
            if not test_model:
                test_model = installed_models[0]
            
            print(f"ğŸ§ª {test_model}ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            payload = {
                "model": test_model,
                "prompt": "Hello! Just say 'AI Ready!' in Korean.",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 10
                }
            }
            
            start_time = time.time()
            response = requests.post(f"{self.ollama_url}/api/generate", 
                                   json=payload, timeout=15)
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json().get("response", "")
                response_time = end_time - start_time
                
                print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
                print(f"   ì‘ë‹µ: {result[:50]}...")
                print(f"   ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ")
                print(f"   ì‚¬ìš© ëª¨ë¸: {test_model}")
                
                # ì†ë„ í‰ê°€
                if response_time < 1.0:
                    print("ğŸš€ ì´ˆê³ ì† ì‘ë‹µ!")
                elif response_time < 3.0:
                    print("âš¡ ë¹ ë¥¸ ì‘ë‹µ!")
                else:
                    print("ğŸŒ ì‘ë‹µì´ ë‹¤ì†Œ ëŠë¦¼")
                
            else:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ”§ LLM ì—°ê²° ë¬¸ì œ í•´ê²°ì‚¬")
    print("Ollama ì„œë²„ ë° ëª¨ë¸ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê³  ìë™ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤")
    print()
    
    fixer = LLMConnectionFixer()
    
    print("ë©”ë‰´:")
    print("1. ë¹ ë¥¸ ì§„ë‹¨ (ë¬¸ì œë§Œ í™•ì¸)")
    print("2. ì¢…í•© í•´ê²° (ì§„ë‹¨ + ìë™ í•´ê²°)")
    print("3. ì„œë²„ ì¬ì‹œì‘ë§Œ")
    print("4. ê¸°ë³¸ ëª¨ë¸ ì„¤ì¹˜")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-4, ê¸°ë³¸ê°’ 2): ").strip() or "2"
    
    if choice == "1":
        issues = fixer.diagnose_connection()
        if issues:
            print(f"\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œë“¤: {issues}")
        else:
            print("\nâœ… ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    elif choice == "2":
        success = fixer.run_comprehensive_fix()
        if success:
            print("\nğŸŠ ì´ì œ LLM AIë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("   python zero_knowledge_ai.py")
        else:
            print("\nğŸ’¡ ìˆ˜ë™ í•´ê²° ë°©ë²•:")
            print("1. CMDì—ì„œ: ollama serve")
            print("2. ë‹¤ë¥¸ CMDì—ì„œ: ollama pull llama3.2:1b")
    
    elif choice == "3":
        fixer.restart_ollama_server()
    
    elif choice == "4":
        fixer.install_basic_models()
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()