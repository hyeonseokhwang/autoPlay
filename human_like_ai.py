#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì˜ì›…ì „ì„¤4 ì§„ì§œ ì¸ê°„í˜• ì¶”ë¡  AI
- ì œë¡œë² ì´ìŠ¤ì—ì„œ ì‹œì‘
- ì‹ ê²½ë§ìœ¼ë¡œ í™”ë©´ ì´í•´
- ì‚¬ëŒì²˜ëŸ¼ ì¶”ë¡ í•˜ê³  íŒë‹¨
- ìˆœìˆ˜ ê²½í—˜ ê¸°ë°˜ í•™ìŠµ
"""

import asyncio
import time
import numpy as np
import cv2
import json
import sqlite3
from datetime import datetime
from collections import deque
import random
from typing import Dict, List, Tuple, Any, Optional
from PIL import ImageGrab
import win32gui
import win32con
import win32api

class NeuralGameVision:
    """ì‹ ê²½ë§ ê¸°ë°˜ ê²Œì„ ì‹œê° ì¸ì‹"""
    
    def __init__(self, input_size: Tuple[int, int] = (64, 64)):
        """ì´ˆê¸°í™”"""
        self.input_size = input_size
        self.last_processed_image = None
        
        # ê°„ë‹¨í•œ CNN êµ¬ì¡° (numpyë¡œ êµ¬í˜„)
        self.conv_weights = []
        self.dense_weights = []
        self._initialize_weights()
        
        # ì¶”ë¡  íˆìŠ¤í† ë¦¬
        self.reasoning_history = deque(maxlen=100)
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        # Conv ë ˆì´ì–´ (3x3 í•„í„° 16ê°œ)
        self.conv_weights = np.random.randn(16, 3, 3, 3) * 0.1
        
        # Dense ë ˆì´ì–´ 
        # 64x64 -> 32x32 (conv+pool) -> flatten -> dense
        self.w1 = np.random.randn(32*32*16, 128) * 0.1
        self.b1 = np.zeros(128)
        
        self.w2 = np.random.randn(128, 64) * 0.1
        self.b2 = np.zeros(64)
        
        # ì¶œë ¥: ì˜ë¯¸ìˆëŠ” íŒ¨í„´ ì¸ì‹
        self.w_out = np.random.randn(64, 32) * 0.1  # 32ê°œ íŒ¨í„´
        self.b_out = np.zeros(32)
        
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if image is None:
            return None
            
        # ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image, self.input_size)
        
        # ì •ê·œí™”
        normalized = resized.astype(np.float32) / 255.0
        
        self.last_processed_image = normalized
        return normalized
    
    def simple_conv2d(self, image: np.ndarray, weight: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ 2D ì»¨ë³¼ë£¨ì…˜"""
        h, w, c = image.shape
        fh, fw = weight.shape[1], weight.shape[2]
        
        # ì¶œë ¥ í¬ê¸°
        oh = h - fh + 1
        ow = w - fw + 1
        
        output = np.zeros((oh, ow))
        
        for y in range(oh):
            for x in range(ow):
                patch = image[y:y+fh, x:x+fw]
                output[y, x] = np.sum(patch * weight[0])  # ì²« ë²ˆì§¸ ì±„ë„ë§Œ
                
        return output
    
    def relu(self, x: np.ndarray) -> np.ndarray:
        """ReLU í™œì„±í™”"""
        return np.maximum(0, x)
    
    def forward_pass(self, image: np.ndarray) -> np.ndarray:
        """ìˆœì „íŒŒ (ì¶”ë¡ )"""
        processed_img = self.preprocess_image(image)
        if processed_img is None:
            return np.zeros(32)
        
        try:
            # Conv ë ˆì´ì–´ (ê°„ë‹¨í™”)
            conv_output = []
            for i in range(min(4, len(self.conv_weights))):  # 4ê°œ í•„í„°ë§Œ
                conv_out = self.simple_conv2d(processed_img, self.conv_weights[i])
                conv_output.append(conv_out)
            
            # Max pooling (2x2)
            pooled_outputs = []
            for conv_out in conv_output:
                h, w = conv_out.shape
                pooled = cv2.resize(conv_out, (w//2, h//2))
                pooled_outputs.append(pooled)
            
            # Flatten
            flattened = np.concatenate([p.flatten() for p in pooled_outputs])
            
            # Dense ë ˆì´ì–´ë“¤
            if len(flattened) > 0:
                # í¬ê¸° ë§ì¶”ê¸°
                target_size = self.w1.shape[0]
                if len(flattened) > target_size:
                    flattened = flattened[:target_size]
                elif len(flattened) < target_size:
                    padded = np.zeros(target_size)
                    padded[:len(flattened)] = flattened
                    flattened = padded
                
                h1 = self.relu(np.dot(flattened, self.w1) + self.b1)
                h2 = self.relu(np.dot(h1, self.w2) + self.b2)
                output = np.dot(h2, self.w_out) + self.b_out
                
                return output
            
        except Exception as e:
            print(f"âš ï¸ ì‹ ê²½ë§ ì˜¤ë¥˜: {e}")
            
        return np.zeros(32)
    
    def interpret_patterns(self, neural_output: np.ndarray) -> Dict[str, Any]:
        """ì‹ ê²½ë§ ì¶œë ¥ì„ ì˜ë¯¸ìˆëŠ” íŒ¨í„´ìœ¼ë¡œ í•´ì„"""
        patterns = {}
        
        # ê° ë‰´ëŸ°ì„ ì˜ë¯¸ìˆëŠ” íŒ¨í„´ìœ¼ë¡œ ë§¤í•‘
        pattern_names = [
            'movement_space', 'ui_element', 'character_sprite', 'background_texture',
            'bright_area', 'dark_area', 'colorful_region', 'text_like',
            'menu_indicator', 'battle_signal', 'item_hint', 'door_passage',
            'enemy_presence', 'interactive_object', 'status_display', 'map_feature',
            'animation_motion', 'popup_window', 'selection_cursor', 'health_indicator',
            'magic_effect', 'treasure_sign', 'npc_character', 'environment_change',
            'quest_marker', 'save_point', 'checkpoint', 'secret_area',
            'danger_zone', 'safe_area', 'exploration_target', 'unknown_pattern'
        ]
        
        for i, activation in enumerate(neural_output):
            if i < len(pattern_names):
                # í™œì„±í™” ê°’ì„ 0-1ë¡œ ì •ê·œí™”
                normalized = max(0, min(1, (activation + 2) / 4))
                patterns[pattern_names[i]] = normalized
        
        return patterns

class HumanLikeReasoning:
    """ì¸ê°„í˜• ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.reasoning_memory = deque(maxlen=50)
        self.curiosity_level = 0.8
        self.confidence_threshold = 0.3
        self.exploration_motivation = 1.0
        
        # ì¶”ë¡  ê°€ì¤‘ì¹˜ (ê²½í—˜ìœ¼ë¡œ í•™ìŠµë¨)
        self.reasoning_weights = {
            'exploration': 1.0,
            'interaction': 0.8,
            'safety': 0.6,
            'novelty': 0.9,
            'pattern_recognition': 0.7
        }
        
    def analyze_situation(self, visual_patterns: Dict[str, float], 
                         action_history: List[str]) -> Dict[str, Any]:
        """ìƒí™© ë¶„ì„ ë° ì¶”ë¡ """
        
        # 1. í˜„ì¬ ìƒí™© ì´í•´
        situation_assessment = self._assess_current_situation(visual_patterns)
        
        # 2. í˜¸ê¸°ì‹¬ ê¸°ë°˜ íƒí—˜ ìš•êµ¬
        curiosity_drive = self._calculate_curiosity(visual_patterns, action_history)
        
        # 3. ìƒí˜¸ì‘ìš© ê°€ëŠ¥ì„± íŒë‹¨
        interaction_potential = self._evaluate_interaction_opportunities(visual_patterns)
        
        # 4. ì•ˆì „ì„± í‰ê°€
        safety_assessment = self._evaluate_safety(visual_patterns)
        
        # 5. ì¢…í•© ì¶”ë¡ 
        reasoning_result = {
            'situation': situation_assessment,
            'curiosity': curiosity_drive,
            'interaction': interaction_potential,
            'safety': safety_assessment,
            'overall_confidence': self._calculate_confidence(visual_patterns),
            'recommended_actions': self._generate_action_recommendations(
                visual_patterns, curiosity_drive, interaction_potential
            ),
            'reasoning_explanation': self._generate_explanation(
                situation_assessment, curiosity_drive, interaction_potential
            )
        }
        
        # ì¶”ë¡  ê¸°ë¡
        self.reasoning_memory.append({
            'timestamp': datetime.now(),
            'patterns': visual_patterns.copy(),
            'reasoning': reasoning_result.copy()
        })
        
        return reasoning_result
    
    def _assess_current_situation(self, patterns: Dict[str, float]) -> Dict[str, Any]:
        """í˜„ì¬ ìƒí™© í‰ê°€"""
        # ì£¼ìš” íŒ¨í„´ë“¤ì˜ ê°•ë„ ë¶„ì„
        ui_strength = patterns.get('ui_element', 0) + patterns.get('menu_indicator', 0)
        exploration_potential = patterns.get('movement_space', 0) + patterns.get('map_feature', 0)
        interaction_signs = patterns.get('interactive_object', 0) + patterns.get('npc_character', 0)
        
        situation_type = 'unknown'
        if ui_strength > 0.5:
            situation_type = 'menu_navigation'
        elif exploration_potential > 0.6:
            situation_type = 'field_exploration'  
        elif interaction_signs > 0.4:
            situation_type = 'interaction_opportunity'
        elif patterns.get('battle_signal', 0) > 0.3:
            situation_type = 'potential_combat'
        
        return {
            'type': situation_type,
            'ui_strength': ui_strength,
            'exploration_potential': exploration_potential,
            'interaction_signs': interaction_signs,
            'complexity': np.std(list(patterns.values()))
        }
    
    def _calculate_curiosity(self, patterns: Dict[str, float], 
                           action_history: List[str]) -> Dict[str, float]:
        """í˜¸ê¸°ì‹¬ ê³„ì‚°"""
        # ìƒˆë¡œìš´ íŒ¨í„´ì— ëŒ€í•œ í˜¸ê¸°ì‹¬
        novelty_score = 0.0
        for pattern_name, activation in patterns.items():
            if activation > 0.3 and pattern_name.endswith(('_target', '_sign', '_hint')):
                novelty_score += activation * self.curiosity_level
        
        # ìµœê·¼ í–‰ë™ì˜ ë‹¤ì–‘ì„±
        recent_actions = action_history[-10:] if len(action_history) >= 10 else action_history
        action_diversity = len(set(recent_actions)) / max(len(recent_actions), 1)
        
        # íƒí—˜í•˜ì§€ ì•Šì€ ì˜ì—­ì— ëŒ€í•œ í˜¸ê¸°ì‹¬
        exploration_urge = patterns.get('unknown_pattern', 0) * 1.5
        
        return {
            'novelty': novelty_score,
            'diversity_seeking': action_diversity,
            'exploration_urge': exploration_urge,
            'total_curiosity': (novelty_score + action_diversity + exploration_urge) / 3
        }
    
    def _evaluate_interaction_opportunities(self, patterns: Dict[str, float]) -> Dict[str, float]:
        """ìƒí˜¸ì‘ìš© ê¸°íšŒ í‰ê°€"""
        interactive_elements = [
            'interactive_object', 'npc_character', 'door_passage', 
            'item_hint', 'save_point', 'treasure_sign'
        ]
        
        interaction_scores = {}
        total_interaction_potential = 0.0
        
        for element in interactive_elements:
            score = patterns.get(element, 0)
            interaction_scores[element] = score
            total_interaction_potential += score
        
        return {
            **interaction_scores,
            'total_potential': total_interaction_potential,
            'highest_priority': max(interaction_scores, key=interaction_scores.get) if interaction_scores else None
        }
    
    def _evaluate_safety(self, patterns: Dict[str, float]) -> Dict[str, float]:
        """ì•ˆì „ì„± í‰ê°€"""
        danger_indicators = patterns.get('danger_zone', 0) + patterns.get('enemy_presence', 0)
        safe_indicators = patterns.get('safe_area', 0) + patterns.get('save_point', 0)
        
        return {
            'danger_level': danger_indicators,
            'safety_level': safe_indicators,
            'overall_safety': safe_indicators - danger_indicators
        }
    
    def _calculate_confidence(self, patterns: Dict[str, float]) -> float:
        """ì¶”ë¡  ì‹ ë¢°ë„ ê³„ì‚°"""
        # íŒ¨í„´ í™œì„±í™”ì˜ ì¼ê´€ì„±
        activations = list(patterns.values())
        if not activations:
            return 0.0
            
        max_activation = max(activations)
        mean_activation = np.mean(activations)
        
        # ëª…í™•í•œ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ë†’ìŒ
        clarity = max_activation - mean_activation
        confidence = min(1.0, clarity * 2)
        
        return confidence
    
    def _generate_action_recommendations(self, patterns: Dict[str, float], 
                                      curiosity: Dict[str, float],
                                      interaction: Dict[str, float]) -> List[Dict[str, Any]]:
        """í–‰ë™ ì¶”ì²œ ìƒì„±"""
        recommendations = []
        
        # í˜¸ê¸°ì‹¬ ê¸°ë°˜ ì¶”ì²œ
        if curiosity['total_curiosity'] > 0.5:
            if patterns.get('movement_space', 0) > 0.4:
                recommendations.append({
                    'action': 'explore_movement',
                    'priority': curiosity['exploration_urge'],
                    'reason': 'í˜¸ê¸°ì‹¬ - ìƒˆë¡œìš´ ì˜ì—­ íƒí—˜'
                })
        
        # ìƒí˜¸ì‘ìš© ê¸°ë°˜ ì¶”ì²œ
        if interaction['total_potential'] > 0.3:
            highest_priority = interaction.get('highest_priority')
            if highest_priority:
                action_map = {
                    'interactive_object': 'interact_object',
                    'npc_character': 'talk_to_npc', 
                    'door_passage': 'enter_door',
                    'item_hint': 'investigate_item'
                }
                
                action = action_map.get(highest_priority, 'interact_general')
                recommendations.append({
                    'action': action,
                    'priority': interaction[highest_priority],
                    'reason': f'ìƒí˜¸ì‘ìš© ê¸°íšŒ - {highest_priority}'
                })
        
        # íƒí—˜ ê¸°ë³¸ ì¶”ì²œ
        if not recommendations:
            recommendations.append({
                'action': 'random_exploration',
                'priority': 0.5,
                'reason': 'ê¸°ë³¸ íƒí—˜ í–‰ë™'
            })
        
        return sorted(recommendations, key=lambda x: x['priority'], reverse=True)
    
    def _generate_explanation(self, situation: Dict, curiosity: Dict, interaction: Dict) -> str:
        """ì¶”ë¡  ê³¼ì • ì„¤ëª… ìƒì„±"""
        explanation_parts = []
        
        explanation_parts.append(f"ìƒí™©: {situation['type']}")
        
        if curiosity['total_curiosity'] > 0.5:
            explanation_parts.append(f"í˜¸ê¸°ì‹¬ ë°œë™ (ê°•ë„: {curiosity['total_curiosity']:.2f})")
            
        if interaction['total_potential'] > 0.3:
            explanation_parts.append(f"ìƒí˜¸ì‘ìš© ê°€ëŠ¥ì„± ë°œê²¬")
            
        return " | ".join(explanation_parts)
    
    def learn_from_experience(self, action_taken: str, outcome_patterns: Dict[str, float], 
                            reward_signal: float) -> None:
        """ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµ"""
        if not self.reasoning_memory:
            return
            
        last_reasoning = self.reasoning_memory[-1]
        
        # ì¶”ì²œí–ˆë˜ í–‰ë™ì˜ ê²°ê³¼ ë¶„ì„
        recommended_actions = last_reasoning['reasoning']['recommended_actions']
        
        for rec in recommended_actions:
            if action_taken in rec['action']:
                # ê²°ê³¼ê°€ ì¢‹ì•˜ìœ¼ë©´ í•´ë‹¹ ì¶”ë¡  íŒ¨í„´ ê°•í™”
                if reward_signal > 0:
                    pattern_type = rec['reason'].split(' - ')[0] if ' - ' in rec['reason'] else 'general'
                    if pattern_type in self.reasoning_weights:
                        self.reasoning_weights[pattern_type] = min(2.0, 
                            self.reasoning_weights[pattern_type] * 1.1)
                        print(f"ğŸ§  í•™ìŠµ: '{pattern_type}' ì¶”ë¡  ê°•í™” â†’ {self.reasoning_weights[pattern_type]:.3f}")
                else:
                    # ê²°ê³¼ê°€ ë‚˜ë¹´ìœ¼ë©´ ì•½í™”
                    pattern_type = rec['reason'].split(' - ')[0] if ' - ' in rec['reason'] else 'general'
                    if pattern_type in self.reasoning_weights:
                        self.reasoning_weights[pattern_type] = max(0.1, 
                            self.reasoning_weights[pattern_type] * 0.9)
                        print(f"ğŸ§  í•™ìŠµ: '{pattern_type}' ì¶”ë¡  ì•½í™” â†’ {self.reasoning_weights[pattern_type]:.3f}")

class IntelligentGameController:
    """ì§€ëŠ¥í˜• ê²Œì„ ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.dosbox_window = None
        self.game_region = None
        
        # í–‰ë™ ë§¤í•‘
        self.action_mappings = {
            'explore_movement': ['left', 'right', 'up', 'down'],
            'interact_object': ['space', 'enter', 'z'],
            'talk_to_npc': ['space', 'enter'],
            'enter_door': ['up', 'space'],
            'investigate_item': ['z', 'space', 'enter'],
            'interact_general': ['space', 'enter', 'z'],
            'random_exploration': ['left', 'right', 'up', 'down', 'space']
        }
        
    def find_game_window(self) -> bool:
        """ê²Œì„ ì°½ ì°¾ê¸°"""
        def enum_callback(hwnd, windows):
            if win32gui.IsWindowVisible(hwnd):
                window_text = win32gui.GetWindowText(hwnd)
                if 'dosbox' in window_text.lower() or 'ED4' in window_text:
                    windows.append(hwnd)
            return True

        windows = []
        win32gui.EnumWindows(enum_callback, windows)
        
        if windows:
            self.dosbox_window = windows[0]
            self.game_region = win32gui.GetWindowRect(self.dosbox_window)
            print(f"ğŸ® ê²Œì„ ì—°ê²°: {self.game_region}")
            return True
        
        return False
    
    def execute_action(self, action_type: str) -> Tuple[bool, str]:
        """í–‰ë™ ì‹¤í–‰"""
        if not self.dosbox_window:
            return False, "ê²Œì„ ì°½ ì—†ìŒ"
            
        # í–‰ë™ íƒ€ì…ì—ì„œ ì‹¤ì œ í‚¤ ì„ íƒ
        possible_keys = self.action_mappings.get(action_type, ['space'])
        selected_key = random.choice(possible_keys)
        
        try:
            win32gui.SetForegroundWindow(self.dosbox_window)
            time.sleep(0.05)
            
            key_map = {
                'left': 0x25, 'right': 0x27, 'up': 0x26, 'down': 0x28,
                'space': 0x20, 'enter': 0x0D, 'z': 0x5A, 'x': 0x58,
                'a': 0x41, 's': 0x53, '1': 0x31, '2': 0x32
            }
            
            if selected_key in key_map:
                vk_code = key_map[selected_key]
                win32api.keybd_event(vk_code, 0, 0, 0)
                time.sleep(0.08)
                win32api.keybd_event(vk_code, 0, win32con.KEYEVENTF_KEYUP, 0)
                return True, selected_key
                
        except Exception as e:
            return False, f"ì˜¤ë¥˜: {e}"
        
        return False, "ì•Œ ìˆ˜ ì—†ëŠ” í‚¤"
    
    def capture_screen(self) -> np.ndarray:
        """í™”ë©´ ìº¡ì²˜"""
        try:
            screenshot = ImageGrab.grab(self.game_region)
            return np.array(screenshot)
        except Exception as e:
            print(f"âŒ í™”ë©´ ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None

class HumanLikeAI:
    """ì¸ê°„í˜• AI ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.vision = NeuralGameVision()
        self.reasoning = HumanLikeReasoning()
        self.controller = IntelligentGameController()
        
        # ê²½í—˜ ì €ì¥
        self.experience_db = ExperienceDatabase()
        self.session_id = f"human_ai_{int(time.time())}"
        
        # ìƒíƒœ ì¶”ì 
        self.action_history = deque(maxlen=100)
        self.step_count = 0
        self.battle_discoveries = 0
        self.total_reward = 0.0
        
        print("ğŸ§  ì¸ê°„í˜• AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def thinking_step(self) -> None:
        """í•œ ë²ˆì˜ ì‚¬ê³  ìŠ¤í…"""
        self.step_count += 1
        
        # 1. í™”ë©´ ê´€ì°°
        screenshot = self.controller.capture_screen()
        if screenshot is None:
            return
        
        # 2. ì‹ ê²½ë§ìœ¼ë¡œ ì‹œê° íŒ¨í„´ ì¸ì‹
        neural_output = self.vision.forward_pass(screenshot)
        visual_patterns = self.vision.interpret_patterns(neural_output)
        
        # 3. ì¸ê°„í˜• ì¶”ë¡ 
        reasoning_result = self.reasoning.analyze_situation(
            visual_patterns, list(self.action_history)
        )
        
        print(f"\nğŸ¤” ìŠ¤í… {self.step_count} - ì¶”ë¡  ì¤‘...")
        print(f"   ğŸ§  {reasoning_result['reasoning_explanation']}")
        print(f"   ğŸ“Š ì‹ ë¢°ë„: {reasoning_result['overall_confidence']:.2f}")
        
        # 4. í–‰ë™ ê²°ì •
        recommendations = reasoning_result['recommended_actions']
        if recommendations:
            chosen_action = recommendations[0]  # ìµœìš°ì„  ì¶”ì²œ
            action_type = chosen_action['action']
            
            print(f"   ğŸ¯ í–‰ë™ ê²°ì •: {action_type} (ì´ìœ : {chosen_action['reason']})")
            
            # 5. í–‰ë™ ì‹¤í–‰
            success, actual_key = self.controller.execute_action(action_type)
            
            if success:
                self.action_history.append(actual_key)
                print(f"   âš¡ ì‹¤í–‰: {actual_key}")
                
                # 6. ê²°ê³¼ ê´€ì°° ë° ë³´ìƒ ê³„ì‚°
                await asyncio.sleep(0.2)  # ê²°ê³¼ ëŒ€ê¸°
                
                next_screenshot = self.controller.capture_screen()
                if next_screenshot is not None:
                    next_neural = self.vision.forward_pass(next_screenshot)
                    next_patterns = self.vision.interpret_patterns(next_neural)
                    
                    # ë³´ìƒ ê³„ì‚° (ê°„ë‹¨í•œ ë³€í™” ê¸°ë°˜)
                    reward = self._calculate_experience_reward(
                        visual_patterns, next_patterns, action_type
                    )
                    self.total_reward += reward
                    
                    # 7. ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµ
                    self.reasoning.learn_from_experience(
                        actual_key, next_patterns, reward
                    )
                    
                    # ì „íˆ¬ ë°œê²¬ ì²´í¬
                    if self._detect_battle_discovery(next_patterns):
                        self.battle_discoveries += 1
                        print(f"   âš”ï¸ ì „íˆ¬ ë°œê²¬! ì´ {self.battle_discoveries}íšŒ")
        
        else:
            print(f"   â“ ì¶”ë¡  ì‹¤íŒ¨ - ëœë¤ í–‰ë™")
            # ë¹„ìƒ ëœë¤ í–‰ë™
            success, actual_key = self.controller.execute_action('random_exploration')
            if success:
                self.action_history.append(actual_key)
    
    def _calculate_experience_reward(self, before_patterns: Dict[str, float],
                                   after_patterns: Dict[str, float], 
                                   action_type: str) -> float:
        """ê²½í—˜ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°"""
        reward = 0.01  # ê¸°ë³¸ ìƒì¡´ ë³´ìƒ
        
        # íŒ¨í„´ ë³€í™” ë³´ìƒ
        for pattern_name in before_patterns:
            before_val = before_patterns.get(pattern_name, 0)
            after_val = after_patterns.get(pattern_name, 0)
            change = abs(after_val - before_val)
            
            if change > 0.1:  # ì˜ë¯¸ìˆëŠ” ë³€í™”
                reward += change * 0.5
        
        # íŠ¹ì • íŒ¨í„´ ë°œê²¬ ë³´ìƒ
        valuable_patterns = ['battle_signal', 'treasure_sign', 'interactive_object', 'npc_character']
        for pattern in valuable_patterns:
            if after_patterns.get(pattern, 0) > 0.4:
                reward += 1.0
        
        return reward
    
    def _detect_battle_discovery(self, patterns: Dict[str, float]) -> bool:
        """ì „íˆ¬ ë°œê²¬ ê°ì§€"""
        battle_indicators = (
            patterns.get('battle_signal', 0) > 0.4 or
            patterns.get('enemy_presence', 0) > 0.3 or
            patterns.get('danger_zone', 0) > 0.5
        )
        return battle_indicators
    
    async def run_human_like_session(self, max_steps: int = 300, target_battles: int = 10) -> None:
        """ì¸ê°„í˜• AI ì„¸ì…˜ ì‹¤í–‰"""
        print("ğŸ§  ì¸ê°„í˜• AI ì„¸ì…˜ ì‹œì‘!")
        print(f"ğŸ¯ ëª©í‘œ: {max_steps}ìŠ¤í…ìœ¼ë¡œ {target_battles}ë²ˆì˜ ì „íˆ¬ ë°œê²¬")
        
        if not self.controller.find_game_window():
            print("âŒ ê²Œì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        start_time = time.time()
        
        while (self.step_count < max_steps and 
               self.battle_discoveries < target_battles):
            
            await self.thinking_step()
            await asyncio.sleep(0.25)  # ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ëŠ” ì‹œê°„
            
            # ì§„í–‰ ìƒí™©
            if self.step_count % 25 == 0:
                elapsed = time.time() - start_time
                print(f"\nğŸ“Š ì§„í–‰ ìƒí™©:")
                print(f"   ğŸ® ìŠ¤í…: {self.step_count}/{max_steps}")
                print(f"   âš”ï¸ ì „íˆ¬ ë°œê²¬: {self.battle_discoveries}/{target_battles}")
                print(f"   ğŸ’° ëˆ„ì  ë³´ìƒ: {self.total_reward:.2f}")
                print(f"   â±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed:.1f}ì´ˆ")
                print(f"   ğŸ§  ì¶”ë¡  ê°€ì¤‘ì¹˜ ë³€í™”:")
                for weight_name, weight_val in self.reasoning.reasoning_weights.items():
                    print(f"      {weight_name}: {weight_val:.3f}")
        
        # ìµœì¢… ê²°ê³¼
        elapsed = time.time() - start_time
        print(f"\nğŸ ì„¸ì…˜ ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ğŸ® ì´ ìŠ¤í…: {self.step_count}")
        print(f"âš”ï¸ ì „íˆ¬ ë°œê²¬: {self.battle_discoveries}/{target_battles}")
        print(f"ğŸ’° ì´ ë³´ìƒ: {self.total_reward:.2f}")
        print(f"ğŸ“ˆ í‰ê·  ë³´ìƒ: {self.total_reward/max(self.step_count, 1):.4f}")
        
        if self.battle_discoveries >= target_battles:
            print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! AIê°€ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì¶”ë¡ í–ˆìŠµë‹ˆë‹¤!")
        else:
            print("ğŸ“š í•™ìŠµ ì§„í–‰ ì¤‘. AIê°€ ê²½í—˜ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤.")

class ExperienceDatabase:
    """ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self, db_path: str = "human_ai_experience.db"):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        """DB ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS human_experiences (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    step_number INTEGER,
                    visual_patterns TEXT,
                    reasoning_result TEXT,
                    action_taken TEXT,
                    reward REAL,
                    timestamp TEXT
                )
            """)

# ì‹¤í–‰
if __name__ == "__main__":
    async def main():
        ai = HumanLikeAI()
        await ai.run_human_like_session(max_steps=300, target_battles=10)
    
    print("ğŸ§  ì§„ì§œ ì¸ê°„í˜• ì¶”ë¡  AI")
    print("=" * 70)
    print("âœ¨ íŠ¹ì§•: ì‹ ê²½ë§ ì‹œê°ì¸ì‹ + ì¸ê°„í˜• ì¶”ë¡  + ìˆœìˆ˜ ê²½í—˜ í•™ìŠµ")
    asyncio.run(main())