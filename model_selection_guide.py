"""
ğŸ§  ìµœì  LLM ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ
ìš©ë„ë³„/ì„±ëŠ¥ë³„ ì¶”ì²œ ëª¨ë¸ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
"""

class ModelRecommendations:
    """ëª¨ë¸ ì¶”ì²œ ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def get_recommendations():
        """ì‚¬ìš© ëª©ì ë³„ ëª¨ë¸ ì¶”ì²œ"""
        
        return {
            "ê²Œì„_ai_ì´ˆë³´ì": {
                "ëª¨ë¸": "llama3.2:3b",
                "ì´ìœ ": "ë¹ ë¥¸ ì†ë„, ì ì€ ë©”ëª¨ë¦¬, ì•ˆì •ì  ì„±ëŠ¥",
                "ì„¤ì¹˜": "ollama pull llama3.2:3b",
                "ë©”ëª¨ë¦¬": "4GB",
                "ì†ë„": "1-2ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­â­",
                "ì í•©í•œ_ê²Œì„": ["í„´ì œ RPG", "ì „ëµ ê²Œì„", "í¼ì¦ ê²Œì„"]
            },
            
            "ê²Œì„_ai_ê³ ì„±ëŠ¥": {
                "ëª¨ë¸": "qwen2.5-coder:7b", 
                "ì´ìœ ": "ë›°ì–´ë‚œ ì¶”ë¡ ëŠ¥ë ¥, íŒ¨í„´ì¸ì‹ íŠ¹í™”, í•œêµ­ì–´ ì§€ì›",
                "ì„¤ì¹˜": "ollama pull qwen2.5-coder:7b",
                "ë©”ëª¨ë¦¬": "8GB",
                "ì†ë„": "2-4ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­â­â­",
                "ì í•©í•œ_ê²Œì„": ["ë³µì¡í•œ RPG", "ì „ëµ ì‹œë®¬ë ˆì´ì…˜", "í¼ì¦ ì–´ë“œë²¤ì²˜"]
            },
            
            "ì‹¤ì‹œê°„_ê²Œì„": {
                "ëª¨ë¸": "llama3.2:1b",
                "ì´ìœ ": "ì´ˆê³ ì† ë°˜ì‘, ìµœì†Œ ë©”ëª¨ë¦¬, ì‹¤ì‹œê°„ ì²˜ë¦¬",
                "ì„¤ì¹˜": "ollama pull llama3.2:1b", 
                "ë©”ëª¨ë¦¬": "2GB",
                "ì†ë„": "0.5-1ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­",
                "ì í•©í•œ_ê²Œì„": ["ì•¡ì…˜ ê²Œì„", "FPS", "ë¦¬ë“¬ ê²Œì„"]
            },
            
            "ì´ë¯¸ì§€_ë¶„ì„": {
                "ëª¨ë¸": "llava:7b",
                "ì´ìœ ": "ë©€í‹°ëª¨ë‹¬, í™”ë©´ ì§ì ‘ ë¶„ì„, ì‹œê°ì  ì´í•´",
                "ì„¤ì¹˜": "ollama pull llava:7b",
                "ë©”ëª¨ë¦¬": "12GB",
                "ì†ë„": "5-8ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­â­â­",
                "ì í•©í•œ_ê²Œì„": ["ëª¨ë“  ê²Œì„ (í™”ë©´ ë¶„ì„)"]
            },
            
            "ai_ë¹„ì„œ_ê¸°ë³¸": {
                "ëª¨ë¸": "qwen2.5-coder:7b",
                "ì´ìœ ": "ë²”ìš©ì„±, ì½”ë”©ëŠ¥ë ¥, í•œêµ­ì–´, í™•ì¥ì„±",
                "ì„¤ì¹˜": "ollama pull qwen2.5-coder:7b",
                "ë©”ëª¨ë¦¬": "8GB", 
                "ì†ë„": "2-4ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­â­â­",
                "ì í•©í•œ_ì‘ì—…": ["ì¼ì •ê´€ë¦¬", "ì½”ë”©ë„ì›€", "ì§ˆë¬¸ì‘ë‹µ", "ì°½ì‘"]
            },
            
            "ai_ë¹„ì„œ_ê³ ê¸‰": {
                "ëª¨ë¸": "qwen2.5-coder:14b",
                "ì´ìœ ": "ìµœê³  ì„±ëŠ¥, ë³µì¡í•œ ì¶”ë¡ , ì „ë¬¸ì  ëŒ€í™”",
                "ì„¤ì¹˜": "ollama pull qwen2.5-coder:14b",
                "ë©”ëª¨ë¦¬": "16GB",
                "ì†ë„": "4-8ì´ˆ", 
                "í’ˆì§ˆ": "â­â­â­â­â­",
                "ì í•©í•œ_ì‘ì—…": ["ë³µì¡í•œ ë¶„ì„", "ì „ë¬¸ ìƒë‹´", "ê³ ê¸‰ í”„ë¡œê·¸ë˜ë°"]
            },
            
            "í•œêµ­ì–´_íŠ¹í™”": {
                "ëª¨ë¸": "eeve-korean:10.8b",
                "ì´ìœ ": "í•œêµ­ì–´ ì™„ë²½ì§€ì›, ë¬¸í™”ì  ë§¥ë½, ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”",
                "ì„¤ì¹˜": "ollama pull eeve-korean:10.8b",
                "ë©”ëª¨ë¦¬": "12GB",
                "ì†ë„": "3-6ì´ˆ",
                "í’ˆì§ˆ": "â­â­â­â­â­",
                "ì í•©í•œ_ì‘ì—…": ["í•œêµ­ì–´ ëŒ€í™”", "ë²ˆì—­", "ì°½ì‘", "êµìœ¡"]
            }
        }

def print_recommendations():
    """ì¶”ì²œ ëª¨ë¸ ì¶œë ¥"""
    
    recs = ModelRecommendations.get_recommendations()
    
    print("ğŸ¯ ìš©ë„ë³„ ìµœì  ëª¨ë¸ ì¶”ì²œ")
    print("=" * 60)
    
    for category, info in recs.items():
        print(f"\nğŸ“‹ {category.replace('_', ' ').title()}")
        print(f"   ğŸ¤– ëª¨ë¸: {info['ëª¨ë¸']}")
        print(f"   ğŸ’¡ ì´ìœ : {info['ì´ìœ ']}")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {info['ë©”ëª¨ë¦¬']}")
        print(f"   â±ï¸ ì†ë„: {info['ì†ë„']}")
        print(f"   â­ í’ˆì§ˆ: {info['í’ˆì§ˆ']}")
        print(f"   ğŸ“¥ ì„¤ì¹˜: {info['ì„¤ì¹˜']}")
        
        if "ì í•©í•œ_ê²Œì„" in info:
            games = ", ".join(info["ì í•©í•œ_ê²Œì„"])
            print(f"   ğŸ® ê²Œì„: {games}")
        
        if "ì í•©í•œ_ì‘ì—…" in info:
            tasks = ", ".join(info["ì í•©í•œ_ì‘ì—…"])
            print(f"   ğŸ’¼ ì‘ì—…: {tasks}")

def create_install_script():
    """ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    
    recs = ModelRecommendations.get_recommendations()
    
    # ë‹¨ê³„ë³„ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
    scripts = {
        "ê¸°ë³¸íŒ©": [
            "llama3.2:3b",      # ë¹ ë¥¸ ê¸°ë³¸ ëª¨ë¸
            "qwen2.5-coder:7b"  # ê³ ì„±ëŠ¥ ë©”ì¸ ëª¨ë¸
        ],
        
        "ê²Œì„íŒ©": [
            "llama3.2:1b",      # ì‹¤ì‹œê°„ìš©
            "llama3.2:3b",      # ê¸°ë³¸ìš©  
            "qwen2.5-coder:7b", # ì „ëµìš©
            "llava:7b"          # ì´ë¯¸ì§€ ë¶„ì„ìš©
        ],
        
        "ë¹„ì„œíŒ©": [
            "qwen2.5-coder:7b",   # ê¸°ë³¸ ë¹„ì„œ
            "qwen2.5-coder:14b",  # ê³ ê¸‰ ë¹„ì„œ  
            "eeve-korean:10.8b"   # í•œêµ­ì–´ íŠ¹í™”
        ],
        
        "í’€íŒ©": [
            "llama3.2:1b",
            "llama3.2:3b", 
            "qwen2.5-coder:7b",
            "qwen2.5-coder:14b",
            "deepseek-coder:6.7b",
            "llava:7b",
            "eeve-korean:10.8b"
        ]
    }
    
    return scripts

# ì¸í„°ë™í‹°ë¸Œ ëª¨ë¸ ì„ íƒê¸°
def interactive_model_selection():
    """ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ë©° ìµœì  ëª¨ë¸ ì„ íƒ"""
    
    print("ğŸ¤– AI ëª¨ë¸ ë§ì¶¤ ì¶”ì²œ ì‹œìŠ¤í…œ")
    print("=" * 40)
    
    # ì‚¬ìš© ëª©ì  ì§ˆë¬¸
    print("\n1ï¸âƒ£ ì£¼ìš” ì‚¬ìš© ëª©ì ì€?")
    print("   1. ê²Œì„ ìë™ í”Œë ˆì´")
    print("   2. AI ë¹„ì„œ (ì¼ì •, ì§ˆë¬¸ë‹µë³€)")
    print("   3. ë‘˜ ë‹¤")
    
    purpose = input("ì„ íƒ (1-3): ").strip()
    
    # ì„±ëŠ¥ vs ì†ë„ ì„ í˜¸ë„
    print("\n2ï¸âƒ£ ë” ì¤‘ìš”í•œ ê²ƒì€?") 
    print("   1. ë¹ ë¥¸ ì†ë„ (1ì´ˆ ë‚´)")
    print("   2. ë†’ì€ í’ˆì§ˆ (3-5ì´ˆ)")
    print("   3. ê· í˜•")
    
    priority = input("ì„ íƒ (1-3): ").strip()
    
    # ì‹œìŠ¤í…œ ì‚¬ì–‘
    print("\n3ï¸âƒ£ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ëŠ”?")
    print("   1. 8GB ì´í•˜")
    print("   2. 16GB")  
    print("   3. 32GB ì´ìƒ")
    
    memory = input("ì„ íƒ (1-3): ").strip()
    
    # ì¶”ì²œ ìƒì„±
    recommendations = []
    
    # ê²Œì„ ìš©ë„
    if purpose in ["1", "3"]:
        if priority == "1":  # ì†ë„ ì¤‘ì‹œ
            recommendations.append("llama3.2:1b")
        elif priority == "2":  # í’ˆì§ˆ ì¤‘ì‹œ
            recommendations.append("qwen2.5-coder:7b")
        else:  # ê· í˜•
            recommendations.append("llama3.2:3b")
    
    # ë¹„ì„œ ìš©ë„
    if purpose in ["2", "3"]:
        if memory == "1":  # 8GB ì´í•˜
            recommendations.append("qwen2.5-coder:7b")
        elif memory == "3":  # 32GB ì´ìƒ
            recommendations.append("qwen2.5-coder:14b")
            recommendations.append("eeve-korean:10.8b")
        else:  # 16GB
            recommendations.append("qwen2.5-coder:7b")
    
    # ì´ë¯¸ì§€ ë¶„ì„ (ê²Œì„ìš©)
    if purpose in ["1", "3"] and memory in ["2", "3"]:
        recommendations.append("llava:7b")
    
    # ì¤‘ë³µ ì œê±°
    recommendations = list(set(recommendations))
    
    print(f"\nğŸ¯ ë‹¹ì‹ ì—ê²Œ ìµœì ì¸ ëª¨ë¸:")
    for i, model in enumerate(recommendations, 1):
        print(f"   {i}. {model}")
    
    print(f"\nğŸ“¥ ì„¤ì¹˜ ëª…ë ¹ì–´:")
    for model in recommendations:
        print(f"   ollama pull {model}")
    
    return recommendations

def estimate_requirements(models):
    """ëª¨ë¸ë³„ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
    
    requirements = {
        "llama3.2:1b": {"ram": 2, "time": 0.5},
        "llama3.2:3b": {"ram": 4, "time": 1.5},
        "qwen2.5-coder:7b": {"ram": 8, "time": 3},
        "qwen2.5-coder:14b": {"ram": 16, "time": 6},
        "deepseek-coder:6.7b": {"ram": 8, "time": 3.5},
        "llava:7b": {"ram": 12, "time": 7},
        "eeve-korean:10.8b": {"ram": 12, "time": 5}
    }
    
    total_ram = sum(requirements.get(model, {"ram": 4})["ram"] for model in models)
    avg_time = sum(requirements.get(model, {"time": 2})["time"] for model in models) / len(models)
    
    return {
        "total_ram_needed": total_ram,
        "avg_response_time": avg_time,
        "recommended_ram": total_ram + 4,  # ì‹œìŠ¤í…œ ì—¬ìœ ë¶„
        "disk_space": len(models) * 3.5,  # ëª¨ë¸ë‹¹ í‰ê·  3.5GB
    }

if __name__ == "__main__":
    print("ğŸ§  LLM ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ")
    print()
    
    while True:
        print("\në©”ë‰´:")
        print("1. ìš©ë„ë³„ ì¶”ì²œ ëª¨ë¸ ë³´ê¸°")
        print("2. ë§ì¶¤ ëª¨ë¸ ì¶”ì²œë°›ê¸°")  
        print("3. ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ë³´ê¸°")
        print("4. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
        
        if choice == "1":
            print_recommendations()
            
        elif choice == "2":
            selected_models = interactive_model_selection()
            reqs = estimate_requirements(selected_models)
            
            print(f"\nğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:")
            print(f"   RAM: {reqs['recommended_ram']}GB ê¶Œì¥")
            print(f"   ë””ìŠ¤í¬: {reqs['disk_space']:.1f}GB")
            print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {reqs['avg_response_time']:.1f}ì´ˆ")
            
        elif choice == "3":
            scripts = create_install_script()
            
            print("\nğŸ“¦ ì„¤ì¹˜ íŒ¨í‚¤ì§€:")
            for package, models in scripts.items():
                print(f"\n{package}:")
                for model in models:
                    print(f"   ollama pull {model}")
                    
        elif choice == "4":
            print("ğŸ‘‹ ì¢‹ì€ AI ê°œë°œ ë˜ì„¸ìš”!")
            break
            
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")