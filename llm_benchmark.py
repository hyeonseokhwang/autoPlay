"""
LLM ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ê²Œì„ AI ë° ë¹„ì„œ ê¸°ëŠ¥ì„ ìœ„í•œ ìµœì  ëª¨ë¸ ì„ íƒ
"""

import time
import requests
import json
import psutil
import sys
from typing import Dict, List, Tuple

class LLMBenchmark:
    """LLM ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.test_scenarios = self.get_test_scenarios()
        self.results = {}
    
    def get_test_scenarios(self):
        """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜"""
        return {
            "game_strategy": {
                "prompt": """
ì˜ì›…ì „ì„¤4ì—ì„œ í˜„ì¬ ìƒí™©:
- í•„ë“œ í™”ë©´, ìºë¦­í„° ë ˆë²¨ 5
- HP: 85/100, MP: 20/30
- ì ì´ ë³´ì´ì§€ ì•ŠìŒ
- ì˜¤ë¥¸ìª½ì— ìˆ², ì™¼ìª½ì— ë§ˆì„

ë‹¤ìŒ í–‰ë™ì„ ì„ íƒí•˜ì„¸ìš”:
1. ìˆ²ìœ¼ë¡œ ê°€ì„œ ì  íƒìƒ‰
2. ë§ˆì„ë¡œ ëŒì•„ê°€ì„œ íšŒë³µ
3. í˜„ì¬ ìœ„ì¹˜ì—ì„œ ëŒ€ê¸°
4. ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰

ì„ íƒê³¼ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.
""",
                "expected_keywords": ["ìˆ²", "íƒìƒ‰", "íšŒë³µ", "HP", "MP"],
                "weight": 0.4  # ê²Œì„ AIì—ì„œ 40% ë¹„ì¤‘
            },
            
            "quick_decision": {
                "prompt": "ì „íˆ¬ ì¤‘! ë¹ ë¥¸ ê²°ì • í•„ìš”. ê³µê²©/ë°©ì–´/ë„ë§ ì¤‘ ì„ íƒí•˜ì„¸ìš”. í•œ ë‹¨ì–´ë¡œ ë‹µí•˜ì„¸ìš”.",
                "expected_keywords": ["ê³µê²©", "ë°©ì–´", "ë„ë§", "attack", "defend", "run"],
                "weight": 0.3  # ë°˜ì‘ì†ë„ 30% ë¹„ì¤‘
            },
            
            "korean_conversation": {
                "prompt": """
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë‹¹ì‹ ì˜ AI ë¹„ì„œì…ë‹ˆë‹¤. 
ì˜¤ëŠ˜ ì¼ì •ì„ ê´€ë¦¬í•´ë“œë¦´ê¹Œìš”? 
ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”.
""",
                "expected_keywords": ["ì•ˆë…•", "ë¹„ì„œ", "ì¼ì •", "ë„ì›€", "ê´€ë¦¬"],
                "weight": 0.2  # ë¹„ì„œ ê¸°ëŠ¥ 20% ë¹„ì¤‘
            },
            
            "logical_reasoning": {
                "prompt": """
ë‹¤ìŒ íŒ¨í„´ì„ ë¶„ì„í•˜ì„¸ìš”:
ì „íˆ¬1: ì 3ë§ˆë¦¬ â†’ ìŠ¹ë¦¬ â†’ ê²½í—˜ì¹˜ 150
ì „íˆ¬2: ì 5ë§ˆë¦¬ â†’ ìŠ¹ë¦¬ â†’ ê²½í—˜ì¹˜ 280  
ì „íˆ¬3: ì 2ë§ˆë¦¬ â†’ ìŠ¹ë¦¬ â†’ ê²½í—˜ì¹˜ ?

ì  2ë§ˆë¦¬ì¼ ë•Œ ì˜ˆìƒ ê²½í—˜ì¹˜ëŠ”?
""",
                "expected_keywords": ["100", "ê³„ì‚°", "íŒ¨í„´", "ë¹„ë¡€"],
                "weight": 0.1  # ë…¼ë¦¬ì  ì¶”ë¡  10% ë¹„ì¤‘
            }
        }
    
    def check_ollama_server(self):
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                models = [m["name"] for m in response.json().get("models", [])]
                print(f"âœ… Ollama ì„œë²„ ì—°ê²°ë¨. ëª¨ë¸ {len(models)}ê°œ ë°œê²¬")
                return models
            else:
                print("âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
                return []
        except Exception as e:
            print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print("\nğŸ”§ í•´ê²° ë°©ë²•:")
            print("1. Ollama ì„¤ì¹˜: https://ollama.ai/")
            print("2. ì„œë²„ ì‹œì‘: ollama serve")
            print("3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3.2")
            return []
    
    def test_model(self, model_name: str) -> Dict:
        """ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        results = {
            "model": model_name,
            "scenarios": {},
            "avg_response_time": 0,
            "memory_usage": 0,
            "total_score": 0,
            "errors": []
        }
        
        total_time = 0
        scenario_count = 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        initial_memory = psutil.virtual_memory().used
        
        for scenario_name, scenario in self.test_scenarios.items():
            try:
                print(f"  ğŸ“ {scenario_name} í…ŒìŠ¤íŠ¸...")
                
                # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                
                response = self.call_ollama(model_name, scenario["prompt"])
                
                end_time = time.time()
                response_time = end_time - start_time
                
                # í’ˆì§ˆ í‰ê°€
                quality_score = self.evaluate_response_quality(
                    response, scenario["expected_keywords"]
                )
                
                # ê°€ì¤‘ì¹˜ ì ìš©í•œ ì ìˆ˜
                weighted_score = quality_score * scenario["weight"]
                
                results["scenarios"][scenario_name] = {
                    "response_time": response_time,
                    "quality_score": quality_score,
                    "weighted_score": weighted_score,
                    "response": response[:100] + "..." if len(response) > 100 else response
                }
                
                total_time += response_time
                scenario_count += 1
                
                print(f"    â±ï¸ {response_time:.2f}ì´ˆ, í’ˆì§ˆ: {quality_score:.2f}")
                
            except Exception as e:
                error_msg = f"{scenario_name}: {str(e)}"
                results["errors"].append(error_msg)
                print(f"    âŒ ì˜¤ë¥˜: {error_msg}")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        final_memory = psutil.virtual_memory().used
        results["memory_usage"] = (final_memory - initial_memory) / (1024**2)  # MB
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        if scenario_count > 0:
            results["avg_response_time"] = total_time / scenario_count
        
        # ì´ì  ê³„ì‚°
        results["total_score"] = sum(
            scenario["weighted_score"] 
            for scenario in results["scenarios"].values()
        ) * 100  # 100ì  ë§Œì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
        
        return results
    
    def call_ollama(self, model_name: str, prompt: str, timeout: int = 30) -> str:
        """Ollama API í˜¸ì¶œ"""
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.9
            }
        }
        
        response = requests.post(
            f"{self.ollama_url}/api/generate", 
            json=payload, 
            timeout=timeout
        )
        
        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")
    
    def evaluate_response_quality(self, response: str, expected_keywords: List[str]) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        if not response:
            return 0.0
        
        response_lower = response.lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_matches = sum(
            1 for keyword in expected_keywords 
            if keyword.lower() in response_lower
        )
        keyword_score = keyword_matches / len(expected_keywords)
        
        # ì‘ë‹µ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        length_score = min(1.0, max(0.1, len(response) / 100))
        
        # í•œêµ­ì–´ ì‘ë‹µ ë³´ë„ˆìŠ¤
        korean_chars = sum(1 for char in response if ord(char) > 127)
        korean_bonus = min(0.2, korean_chars / len(response)) if response else 0
        
        total_score = (keyword_score * 0.6) + (length_score * 0.3) + korean_bonus
        return min(1.0, total_score)
    
    def run_benchmark(self, models_to_test: List[str] = None):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        print("ğŸš€ LLM ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        print("=" * 50)
        
        # ì„œë²„ í™•ì¸
        available_models = self.check_ollama_server()
        if not available_models:
            return
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ê²°ì •
        if models_to_test is None:
            models_to_test = available_models
        else:
            # ì‚¬ìš©ì ì§€ì • ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            models_to_test = [m for m in models_to_test if m in available_models]
        
        if not models_to_test:
            print("âŒ í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {models_to_test}")
        
        # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
        for model in models_to_test:
            self.results[model] = self.test_model(model)
        
        # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
        self.analyze_results()
        self.save_results()
    
    def analyze_results(self):
        """ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥"""
        
        print("\n" + "="*60)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„")
        print("="*60)
        
        if not self.results:
            print("âŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëª¨ë¸ë³„ ì ìˆ˜ ì •ë ¬
        sorted_models = sorted(
            self.results.items(),
            key=lambda x: x[1]["total_score"],
            reverse=True
        )
        
        print("\nğŸ† ì¢…í•© ìˆœìœ„:")
        for i, (model, result) in enumerate(sorted_models, 1):
            score = result["total_score"]
            time_avg = result["avg_response_time"]
            memory = result["memory_usage"]
            
            print(f"{i}. {model}")
            print(f"   ğŸ“ˆ ì¢…í•©ì ìˆ˜: {score:.1f}/100")
            print(f"   â±ï¸ í‰ê· ì‘ë‹µ: {time_avg:.2f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {memory:.1f}MB")
            
            if result["errors"]:
                print(f"   âš ï¸ ì˜¤ë¥˜: {len(result['errors'])}ê°œ")
            print()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸
        print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì„±ëŠ¥:")
        
        categories = {
            "ë¹ ë¥¸ ë°˜ì‘": ("avg_response_time", False),  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            "ê²Œì„ ì „ëµ": ("scenarios.game_strategy.quality_score", True),
            "í•œêµ­ì–´ ëŒ€í™”": ("scenarios.korean_conversation.quality_score", True),
            "ë…¼ë¦¬ì  ì¶”ë¡ ": ("scenarios.logical_reasoning.quality_score", True)
        }
        
        for category, (metric, higher_better) in categories.items():
            try:
                best_model = self.find_best_in_category(metric, higher_better)
                if best_model:
                    model_name, value = best_model
                    print(f"   {category}: {model_name} ({value:.2f})")
            except:
                print(f"   {category}: ë°ì´í„° ì—†ìŒ")
        
        # ì¶”ì²œ ëª¨ë¸
        print("\nğŸ’¡ ì¶”ì²œ:")
        self.recommend_models(sorted_models)
    
    def find_best_in_category(self, metric: str, higher_better: bool = True):
        """ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸ ì°¾ê¸°"""
        valid_results = []
        
        for model, result in self.results.items():
            try:
                # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼
                value = result
                for key in metric.split('.'):
                    value = value[key]
                
                valid_results.append((model, value))
            except (KeyError, TypeError):
                continue
        
        if not valid_results:
            return None
        
        return max(valid_results, key=lambda x: x[1] if higher_better else -x[1])
    
    def recommend_models(self, sorted_models):
        """ì‚¬ìš© ëª©ì ë³„ ëª¨ë¸ ì¶”ì²œ"""
        
        if len(sorted_models) == 0:
            print("   ì¶”ì²œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 1ìœ„ ëª¨ë¸ (ì¢…í•©)
        best_overall = sorted_models[0]
        print(f"   ğŸ¥‡ ì¢…í•© ìµœê³ : {best_overall[0]}")
        
        # ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
        fast_models = sorted(
            [(m, r) for m, r in self.results.items() if r["avg_response_time"] > 0],
            key=lambda x: x[1]["avg_response_time"]
        )
        
        if fast_models:
            print(f"   âš¡ ì‹¤ì‹œê°„ ê²Œì„ìš©: {fast_models[0][0]} ({fast_models[0][1]['avg_response_time']:.2f}ì´ˆ)")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëª¨ë¸
        memory_efficient = sorted(
            [(m, r) for m, r in self.results.items() if r["memory_usage"] > 0],
            key=lambda x: x[1]["memory_usage"]
        )
        
        if memory_efficient:
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨: {memory_efficient[0][0]} ({memory_efficient[0][1]['memory_usage']:.1f}MB)")
    
    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"llm_benchmark_{int(time.time())}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ§  LLM ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("ê²Œì„ AI ë° AI ë¹„ì„œë¥¼ ìœ„í•œ ìµœì  ëª¨ë¸ ì„ íƒ")
    print()
    
    benchmark = LLMBenchmark()
    
    # ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ (ì—†ìœ¼ë©´ ì„¤ì¹˜ëœ ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸)
    preferred_models = [
        "llama3.2:1b",
        "llama3.2:3b", 
        "qwen2.5-coder:7b",
        "deepseek-coder:6.7b",
        "llava:7b"
    ]
    
    print("í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:")
    for name, scenario in benchmark.test_scenarios.items():
        print(f"  - {name} (ê°€ì¤‘ì¹˜: {scenario['weight']*100:.0f}%)")
    
    print(f"\nìš°ì„  í…ŒìŠ¤íŠ¸í•  ëª¨ë¸: {preferred_models}")
    print("\nì‹œì‘í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš” (ë˜ëŠ” Ctrl+Cë¡œ ì¢…ë£Œ)...")
    
    try:
        input()
        benchmark.run_benchmark(preferred_models)
        
        print("\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ adaptive_hero_ai.py ìˆ˜ì •")
        print("2. ì„ íƒëœ ëª¨ë¸ë¡œ ê²Œì„ AI í…ŒìŠ¤íŠ¸")
        print("3. ì„±ëŠ¥ì— ë”°ë¼ ì¶”ê°€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ë²¤ì¹˜ë§ˆí¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()