#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì˜ì›…ì „ì„¤4 ì§„ì •í•œ AI í•™ìŠµ ì‹œìŠ¤í…œ
- ì‹¤ì œ ê°•í™”í•™ìŠµ ê¸°ë°˜
- ê²½í—˜ ë°ì´í„° ì¶•ì 
- ììœ¨ì  íŒë‹¨ê³¼ ì¶”ë¡ 
- ìŠ¤ìŠ¤ë¡œ ë°œì „í•˜ëŠ” AI
"""

import asyncio
import time
import random
import numpy as np
import cv2
import json
import sqlite3
from datetime import datetime
from collections import deque
from typing import Dict, List, Tuple, Any
from PIL import ImageGrab
import win32gui
import win32con
import win32api

class GameExperience:
    """ê²Œì„ ê²½í—˜ ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(self, state: Dict, action: str, reward: float, next_state: Dict, info: Dict = None):
        self.state = state
        self.action = action
        self.reward = reward
        self.next_state = next_state
        self.timestamp = datetime.now()
        self.info = info or {}
        
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'state': self.state,
            'action': self.action,
            'reward': self.reward,
            'next_state': self.next_state,
            'timestamp': self.timestamp.isoformat(),
            'info': self.info
        }

class GameStateAnalyzer:
    """ê²Œì„ ìƒíƒœ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.previous_states = deque(maxlen=10)
        self.state_features = {}
        
    def extract_features(self, screenshot: np.ndarray) -> Dict[str, float]:
        """ìŠ¤í¬ë¦°ìƒ·ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        if screenshot is None:
            return {}
            
        try:
            # ê¸°ë³¸ í†µê³„
            brightness = np.mean(screenshot)
            contrast = np.std(screenshot)
            
            # HSV ë³€í™˜
            hsv = cv2.cvtColor(screenshot, cv2.COLOR_RGB2HSV)
            
            # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨
            hist_h = cv2.calcHist([hsv], [0], None, [180], [0, 180])
            hist_s = cv2.calcHist([hsv], [1], None, [256], [0, 256])
            hist_v = cv2.calcHist([hsv], [2], None, [256], [0, 256])
            
            # ìƒ‰ìƒ ë¶„í¬
            red_mask1 = cv2.inRange(hsv, (0, 50, 50), (10, 255, 255))
            red_mask2 = cv2.inRange(hsv, (170, 50, 50), (180, 255, 255))
            blue_mask = cv2.inRange(hsv, (100, 50, 50), (130, 255, 255))
            green_mask = cv2.inRange(hsv, (40, 50, 50), (80, 255, 255))
            yellow_mask = cv2.inRange(hsv, (20, 50, 50), (40, 255, 255))
            
            total_pixels = screenshot.shape[0] * screenshot.shape[1]
            
            # ì—£ì§€ ê²€ì¶œ
            gray = cv2.cvtColor(screenshot, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / total_pixels
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            texture = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            features = {
                'brightness': brightness,
                'contrast': contrast,
                'red_ratio': (np.sum(red_mask1) + np.sum(red_mask2)) / total_pixels,
                'blue_ratio': np.sum(blue_mask) / total_pixels,
                'green_ratio': np.sum(green_mask) / total_pixels,
                'yellow_ratio': np.sum(yellow_mask) / total_pixels,
                'edge_density': edge_density,
                'texture': texture,
                'hue_entropy': self._calculate_entropy(hist_h),
                'saturation_entropy': self._calculate_entropy(hist_s),
                'value_entropy': self._calculate_entropy(hist_v)
            }
            
            return features
            
        except Exception as e:
            print(f"âŒ íŠ¹ì„± ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}
    
    def _calculate_entropy(self, histogram: np.ndarray) -> float:
        """íˆìŠ¤í† ê·¸ë¨ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        histogram = histogram.flatten()
        histogram = histogram[histogram > 0]
        if len(histogram) == 0:
            return 0.0
        
        probabilities = histogram / np.sum(histogram)
        return -np.sum(probabilities * np.log2(probabilities))
    
    def detect_state_change(self, current_features: Dict[str, float]) -> Tuple[bool, float]:
        """ìƒíƒœ ë³€í™” ê°ì§€"""
        if not self.previous_states:
            self.previous_states.append(current_features)
            return False, 0.0
            
        prev_features = self.previous_states[-1]
        
        # ì£¼ìš” íŠ¹ì„±ë“¤ì˜ ë³€í™” ê³„ì‚°
        change_score = 0.0
        important_features = ['brightness', 'red_ratio', 'blue_ratio', 'yellow_ratio', 'edge_density']
        
        for feature in important_features:
            if feature in prev_features and feature in current_features:
                diff = abs(current_features[feature] - prev_features[feature])
                change_score += diff
        
        self.previous_states.append(current_features)
        
        # ì„ê³„ê°’ ì´ìƒì´ë©´ ìƒíƒœ ë³€í™”ë¡œ íŒì •
        significant_change = change_score > 0.1
        
        return significant_change, change_score

class QLearningAgent:
    """Q-Learning ê¸°ë°˜ ê²Œì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self, actions: List[str], learning_rate: float = 0.1, 
                 discount_factor: float = 0.95, epsilon: float = 0.3):
        self.actions = actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon  # íƒí—˜ í™•ë¥ 
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        
        # Q-í…Œì´ë¸” (ìƒíƒœ-í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜)
        self.q_table = {}
        
        # í•™ìŠµ í†µê³„
        self.total_episodes = 0
        self.total_rewards = 0
        self.episode_rewards = []
        
    def state_to_key(self, state: Dict[str, float]) -> str:
        """ìƒíƒœë¥¼ Q-í…Œì´ë¸” í‚¤ë¡œ ë³€í™˜"""
        # ì—°ì†ê°’ì„ ì´ì‚°í™”
        key_parts = []
        
        for feature, value in sorted(state.items()):
            if isinstance(value, (int, float)):
                # ê°’ì„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ì‚°í™”
                if feature == 'brightness':
                    bucket = int(value // 20)  # 0-19, 20-39, ...
                elif feature.endswith('_ratio'):
                    bucket = int(value * 10)   # 0.0-0.1 -> 0, 0.1-0.2 -> 1, ...
                else:
                    bucket = int(value // 10)
                
                key_parts.append(f"{feature}:{bucket}")
        
        return "|".join(key_parts[:6])  # ì²˜ìŒ 6ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©
    
    def get_action(self, state: Dict[str, float]) -> str:
        """ìƒíƒœì— ëŒ€í•œ ìµœì  í–‰ë™ ì„ íƒ (Îµ-greedy)"""
        state_key = self.state_to_key(state)
        
        # ìƒˆë¡œìš´ ìƒíƒœë©´ Qê°’ ì´ˆê¸°í™”
        if state_key not in self.q_table:
            self.q_table[state_key] = {action: 0.0 for action in self.actions}
        
        # Îµ-greedy ì •ì±…
        if random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ í–‰ë™
            action = random.choice(self.actions)
            exploration = True
        else:
            # í™œìš©: ìµœëŒ€ Qê°’ í–‰ë™
            q_values = self.q_table[state_key]
            max_q = max(q_values.values())
            best_actions = [action for action, q in q_values.items() if q == max_q]
            action = random.choice(best_actions)
            exploration = False
        
        return action
    
    def update_q_value(self, state: Dict[str, float], action: str, reward: float, 
                      next_state: Dict[str, float]) -> None:
        """Qê°’ ì—…ë°ì´íŠ¸ (Q-Learning)"""
        state_key = self.state_to_key(state)
        next_state_key = self.state_to_key(next_state)
        
        # Q-í…Œì´ë¸” ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0.0 for a in self.actions}
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0.0 for a in self.actions}
        
        # í˜„ì¬ Qê°’
        current_q = self.q_table[state_key][action]
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Qê°’
        max_next_q = max(self.q_table[next_state_key].values())
        
        # Q-Learning ì—…ë°ì´íŠ¸ ê³µì‹
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state_key][action] = new_q
        
        print(f"ğŸ“š Qí•™ìŠµ: {action} | ë³´ìƒ:{reward:.2f} | Q:{current_q:.3f}â†’{new_q:.3f}")
    
    def decay_epsilon(self) -> None:
        """íƒí—˜ í™•ë¥  ê°ì†Œ"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def get_learning_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return {
            'total_states': len(self.q_table),
            'epsilon': self.epsilon,
            'total_episodes': self.total_episodes,
            'avg_reward': np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0.0,
            'q_table_size': len(self.q_table)
        }

class ExperienceDatabase:
    """ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self, db_path: str = "hero4_experience.db"):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS experiences (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    state TEXT NOT NULL,
                    action TEXT NOT NULL,
                    reward REAL NOT NULL,
                    next_state TEXT NOT NULL,
                    info TEXT,
                    session_id TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS learning_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_time TEXT NOT NULL,
                    end_time TEXT,
                    total_actions INTEGER DEFAULT 0,
                    total_reward REAL DEFAULT 0.0,
                    battles_found INTEGER DEFAULT 0
                )
            """)
    
    def save_experience(self, experience: GameExperience, session_id: str) -> None:
        """ê²½í—˜ ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO experiences 
                (timestamp, state, action, reward, next_state, info, session_id)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                experience.timestamp.isoformat(),
                json.dumps(experience.state),
                experience.action,
                experience.reward,
                json.dumps(experience.next_state),
                json.dumps(experience.info),
                session_id
            ))
    
    def get_recent_experiences(self, limit: int = 1000) -> List[GameExperience]:
        """ìµœê·¼ ê²½í—˜ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT state, action, reward, next_state, timestamp, info
                FROM experiences 
                ORDER BY timestamp DESC 
                LIMIT ?
            """, (limit,))
            
            experiences = []
            for row in cursor:
                exp = GameExperience(
                    state=json.loads(row[0]),
                    action=row[1],
                    reward=row[2],
                    next_state=json.loads(row[3]),
                    info=json.loads(row[5]) if row[5] else {}
                )
                experiences.append(exp)
            
            return experiences

class LearningHero4AI:
    """í•™ìŠµí•˜ëŠ” ì˜ì›…ì „ì„¤4 AI"""
    
    def __init__(self):
        # ê²Œì„ ì—°ê²°
        self.dosbox_window = None
        self.game_region = None
        
        # AI ì»´í¬ë„ŒíŠ¸ë“¤
        self.state_analyzer = GameStateAnalyzer()
        self.actions = ['left', 'right', 'up', 'down', 'space', 'enter', 'z', 'x', 'a', 's', '1', '2']
        self.q_agent = QLearningAgent(self.actions)
        self.experience_db = ExperienceDatabase()
        
        # í•™ìŠµ ìƒíƒœ
        self.session_id = f"session_{int(time.time())}"
        self.current_state = {}
        self.last_action = None
        self.last_screenshot = None
        self.step_count = 0
        self.battle_count = 0
        self.total_reward = 0.0
        
        # ë³´ìƒ ì‹œìŠ¤í…œ
        self.reward_calculator = RewardCalculator()
        
        print("ğŸ§  í•™ìŠµí•˜ëŠ” ì˜ì›…ì „ì„¤4 AI ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ†” ì„¸ì…˜ ID: {self.session_id}")
    
    def find_game_window(self) -> bool:
        """ê²Œì„ ì°½ ì°¾ê¸°"""
        def enum_callback(hwnd, windows):
            if win32gui.IsWindowVisible(hwnd):
                window_text = win32gui.GetWindowText(hwnd)
                if 'dosbox' in window_text.lower() or 'ED4' in window_text:
                    windows.append(hwnd)
            return True

        windows = []
        win32gui.EnumWindows(enum_callback, windows)
        
        if windows:
            self.dosbox_window = windows[0]
            self.game_region = win32gui.GetWindowRect(self.dosbox_window)
            print(f"ğŸ® ê²Œì„ ì—°ê²°: {self.game_region}")
            return True
        
        return False
    
    def capture_game_screen(self) -> np.ndarray:
        """ê²Œì„ í™”ë©´ ìº¡ì²˜"""
        try:
            screenshot = ImageGrab.grab(self.game_region)
            self.last_screenshot = screenshot
            return np.array(screenshot)
        except Exception as e:
            print(f"âŒ í™”ë©´ ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None
    
    def send_action(self, action: str) -> bool:
        """ê²Œì„ì— í–‰ë™ ì „ì†¡"""
        if not self.dosbox_window:
            return False
        
        try:
            win32gui.SetForegroundWindow(self.dosbox_window)
            time.sleep(0.05)
            
            key_map = {
                'left': 0x25, 'right': 0x27, 'up': 0x26, 'down': 0x28,
                'space': 0x20, 'enter': 0x0D, 'z': 0x5A, 'x': 0x58,
                'a': 0x41, 's': 0x53, '1': 0x31, '2': 0x32
            }
            
            if action in key_map:
                vk_code = key_map[action]
                win32api.keybd_event(vk_code, 0, 0, 0)
                time.sleep(0.08)
                win32api.keybd_event(vk_code, 0, win32con.KEYEVENTF_KEYUP, 0)
                return True
                
        except Exception as e:
            print(f"âŒ í–‰ë™ ì „ì†¡ ì‹¤íŒ¨: {e}")
        
        return False
    
    async def learning_step(self) -> None:
        """í•œ ë²ˆì˜ í•™ìŠµ ìŠ¤í…"""
        self.step_count += 1
        
        # 1. í˜„ì¬ ìƒíƒœ ê´€ì°°
        screenshot = self.capture_game_screen()
        if screenshot is None:
            return
            
        current_features = self.state_analyzer.extract_features(screenshot)
        if not current_features:
            return
        
        # 2. ì´ì „ ê²½í—˜ì´ ìˆë‹¤ë©´ í•™ìŠµ ì—…ë°ì´íŠ¸
        if self.current_state and self.last_action:
            # ë³´ìƒ ê³„ì‚°
            reward = self.reward_calculator.calculate_reward(
                self.current_state, current_features, self.last_action
            )
            self.total_reward += reward
            
            # Q-Learning ì—…ë°ì´íŠ¸
            self.q_agent.update_q_value(
                self.current_state, self.last_action, reward, current_features
            )
            
            # ê²½í—˜ ì €ì¥
            experience = GameExperience(
                self.current_state, self.last_action, reward, current_features,
                {'step': self.step_count, 'battle_count': self.battle_count}
            )
            self.experience_db.save_experience(experience, self.session_id)
        
        # 3. ë‹¤ìŒ í–‰ë™ ì„ íƒ
        action = self.q_agent.get_action(current_features)
        
        # 4. í–‰ë™ ì‹¤í–‰
        success = self.send_action(action)
        
        if success:
            print(f"ğŸ® ìŠ¤í… {self.step_count}: {action} | Îµ={self.q_agent.epsilon:.3f}")
            
            # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
            self.current_state = current_features.copy()
            self.last_action = action
            
            # 6. ì „íˆ¬ ê°ì§€
            if self._detect_battle(current_features):
                self.battle_count += 1
                print(f"âš”ï¸ ì „íˆ¬ ê°ì§€! ì´ {self.battle_count}íšŒ")
        
        # 7. íƒí—˜ í™•ë¥  ê°ì†Œ
        if self.step_count % 10 == 0:
            self.q_agent.decay_epsilon()
    
    def _detect_battle(self, features: Dict[str, float]) -> bool:
        """ì „íˆ¬ ê°ì§€ (í•™ìŠµ ê°€ëŠ¥í•œ ë°©ì‹)"""
        # ì—¬ëŸ¬ ì¡°ê±´ì„ ì¡°í•©í•´ì„œ ì „íˆ¬ ê°€ëŠ¥ì„± íŒë‹¨
        battle_indicators = 0
        
        if features.get('red_ratio', 0) > 0.05:
            battle_indicators += 1
        if features.get('blue_ratio', 0) > 0.08:
            battle_indicators += 1
        if features.get('yellow_ratio', 0) > 0.03:
            battle_indicators += 1
        if features.get('brightness', 0) > 80:
            battle_indicators += 1
        if features.get('contrast', 0) > 50:
            battle_indicators += 1
        
        return battle_indicators >= 3
    
    async def run_learning_session(self, max_steps: int = 500, target_battles: int = 15) -> None:
        """í•™ìŠµ ì„¸ì…˜ ì‹¤í–‰"""
        print("ğŸš€ í•™ìŠµ ì„¸ì…˜ ì‹œì‘!")
        print(f"ğŸ¯ ëª©í‘œ: {max_steps}ìŠ¤í… ë‚´ì— {target_battles}íšŒ ì „íˆ¬")
        
        if not self.find_game_window():
            print("âŒ ê²Œì„ ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        start_time = time.time()
        
        while self.step_count < max_steps and self.battle_count < target_battles:
            await self.learning_step()
            await asyncio.sleep(0.15)  # ì ë‹¹í•œ ì†ë„
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if self.step_count % 50 == 0:
                stats = self.q_agent.get_learning_stats()
                elapsed = time.time() - start_time
                sps = self.step_count / elapsed if elapsed > 0 else 0
                
                print(f"ğŸ“Š ì§„í–‰: {self.step_count}/{max_steps} | ì „íˆ¬:{self.battle_count}/{target_battles}")
                print(f"   ğŸ§  ìƒíƒœ:{stats['total_states']} | Îµ:{stats['epsilon']:.3f} | ë³´ìƒ:{self.total_reward:.2f} | {sps:.1f}sps")
        
        # ê²°ê³¼ ìš”ì•½
        elapsed = time.time() - start_time
        print(f"\nğŸ í•™ìŠµ ì„¸ì…˜ ì™„ë£Œ!")
        print(f"â±ï¸ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"ğŸ® ìŠ¤í…: {self.step_count}")
        print(f"âš”ï¸ ì „íˆ¬: {self.battle_count}")
        print(f"ğŸ’° ì´ ë³´ìƒ: {self.total_reward:.2f}")
        
        final_stats = self.q_agent.get_learning_stats()
        print(f"ğŸ§  í•™ìŠµëœ ìƒíƒœ: {final_stats['total_states']}ê°œ")
        print(f"ğŸ” ìµœì¢… íƒí—˜ë¥ : {final_stats['epsilon']:.3f}")
        
        if self.battle_count >= target_battles:
            print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! AIê°€ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!")
        else:
            print("ğŸ“ˆ ë¶€ë¶„ ì„±ê³µ. AIê°€ ê²½í—˜ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤.")

class RewardCalculator:
    """ë³´ìƒ ê³„ì‚°ê¸°"""
    
    def __init__(self):
        self.previous_features = {}
    
    def calculate_reward(self, prev_state: Dict[str, float], 
                        current_state: Dict[str, float], action: str) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        # 1. ê¸°ë³¸ í–‰ë™ ë³´ìƒ (ìƒì¡´)
        reward += 0.01
        
        # 2. í™”ë©´ ë³€í™” ë³´ìƒ (íƒí—˜)
        change_reward = self._calculate_change_reward(prev_state, current_state)
        reward += change_reward
        
        # 3. ì „íˆ¬ ê´€ë ¨ ë³´ìƒ
        battle_reward = self._calculate_battle_reward(current_state)
        reward += battle_reward
        
        # 4. íƒí—˜ ë³´ìƒ
        exploration_reward = self._calculate_exploration_reward(current_state, action)
        reward += exploration_reward
        
        # 5. íŒ¨ë„í‹°
        penalty = self._calculate_penalty(prev_state, current_state)
        reward -= penalty
        
        return reward
    
    def _calculate_change_reward(self, prev_state: Dict, current_state: Dict) -> float:
        """í™”ë©´ ë³€í™” ë³´ìƒ"""
        if not prev_state:
            return 0.0
        
        change_score = 0.0
        for key in ['brightness', 'red_ratio', 'blue_ratio', 'edge_density']:
            if key in prev_state and key in current_state:
                diff = abs(current_state[key] - prev_state[key])
                change_score += diff
        
        return min(change_score * 2.0, 0.5)  # ìµœëŒ€ 0.5ì 
    
    def _calculate_battle_reward(self, current_state: Dict) -> float:
        """ì „íˆ¬ ìƒí™© ë³´ìƒ"""
        battle_score = 0.0
        
        # ì „íˆ¬ ê´€ë ¨ ìƒ‰ìƒë“¤ì— ëŒ€í•œ ë³´ìƒ
        red_ratio = current_state.get('red_ratio', 0)
        blue_ratio = current_state.get('blue_ratio', 0) 
        yellow_ratio = current_state.get('yellow_ratio', 0)
        
        if red_ratio > 0.05:
            battle_score += 2.0
        if blue_ratio > 0.08:
            battle_score += 1.5
        if yellow_ratio > 0.03:
            battle_score += 1.0
            
        return battle_score
    
    def _calculate_exploration_reward(self, current_state: Dict, action: str) -> float:
        """íƒí—˜ ë³´ìƒ"""
        # ë‹¤ì–‘í•œ í–‰ë™ì— ëŒ€í•œ ì‘ì€ ë³´ìƒ
        action_rewards = {
            'left': 0.1, 'right': 0.1, 'up': 0.05, 'down': 0.05,
            'space': 0.2, 'enter': 0.15, 'z': 0.1, 'a': 0.1
        }
        
        return action_rewards.get(action, 0.0)
    
    def _calculate_penalty(self, prev_state: Dict, current_state: Dict) -> float:
        """íŒ¨ë„í‹° ê³„ì‚°"""
        penalty = 0.0
        
        # ë„ˆë¬´ ì–´ë‘ìš´ í™”ë©´ íŒ¨ë„í‹°
        brightness = current_state.get('brightness', 0)
        if brightness < 10:
            penalty += 0.5
        
        # ë³€í™” ì—†ìŒ íŒ¨ë„í‹° (ì •ì²´)
        if prev_state:
            total_change = sum(abs(current_state.get(k, 0) - prev_state.get(k, 0)) 
                             for k in ['brightness', 'red_ratio', 'blue_ratio'])
            if total_change < 0.01:
                penalty += 0.2
        
        return penalty

# ì‹¤í–‰
if __name__ == "__main__":
    async def main():
        ai = LearningHero4AI()
        await ai.run_learning_session(max_steps=500, target_battles=15)
    
    print("ğŸ§  ì§„ì •í•œ AI í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("ğŸ¯ íŠ¹ì§•: ê°•í™”í•™ìŠµ, ê²½í—˜ ì¶•ì , ììœ¨ íŒë‹¨")
    asyncio.run(main())