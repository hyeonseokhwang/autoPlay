"""
ê°•í™”í•™ìŠµ ê¸°ë°˜ ììœ¨ ì§„í™” AI
DQN (Deep Q-Network)ì„ ì‚¬ìš©í•œ ê²Œì„ AI í•™ìŠµ ì‹œìŠ¤í…œ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import cv2

class DQNetwork(nn.Module):
    """Deep Q-Network ëª¨ë¸"""
    
    def __init__(self, input_shape=(84, 84, 4), num_actions=8):
        super(DQNetwork, self).__init__()
        
        # CNN ë ˆì´ì–´ë“¤
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        
        # Fully Connected ë ˆì´ì–´ë“¤
        self.fc1 = nn.Linear(7 * 7 * 64, 512)
        self.fc2 = nn.Linear(512, num_actions)
        
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        
        x = x.view(x.size(0), -1)  # Flatten
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

class ReinforcementLearningAI:
    """ê°•í™”í•™ìŠµ ê¸°ë°˜ ê²Œì„ AI"""
    
    def __init__(self, num_actions=8):
        """
        Actions:
        0: move_left, 1: move_right, 2: move_up, 3: move_down
        4: attack, 5: defend, 6: wait, 7: retreat
        """
        self.num_actions = num_actions
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # DQN ë„¤íŠ¸ì›Œí¬
        self.q_network = DQNetwork(num_actions=num_actions).to(self.device)
        self.target_network = DQNetwork(num_actions=num_actions).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-4)
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´ ë²„í¼
        self.replay_buffer = deque(maxlen=50000)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.batch_size = 32
        self.gamma = 0.99  # í• ì¸ ì¸ìˆ˜
        self.epsilon = 1.0  # íƒí—˜ë¥ 
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
        # í•™ìŠµ ê´€ë ¨
        self.learn_step = 0
        self.target_update_frequency = 1000
        
        # í”„ë ˆì„ ìŠ¤íƒ (ìƒíƒœ ì´ë ¥)
        self.frame_stack = deque(maxlen=4)
        
        # ë³´ìƒ ì²´ê³„
        self.reward_system = {
            "battle_win": 100,
            "battle_loss": -50,
            "find_enemy": 10,
            "explore_new_area": 5,
            "idle": -1,
            "death": -100
        }
    
    def preprocess_state(self, screen):
        """í™”ë©´ì„ ì‹ ê²½ë§ ì…ë ¥ìœ¼ë¡œ ì „ì²˜ë¦¬"""
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(screen, cv2.COLOR_BGR2GRAY)
        
        # í¬ê¸° ì¡°ì •
        resized = cv2.resize(gray, (84, 84))
        
        # ì •ê·œí™”
        normalized = resized / 255.0
        
        return normalized
    
    def get_state(self, screen):
        """í˜„ì¬ ìƒíƒœ ìƒì„± (4í”„ë ˆì„ ìŠ¤íƒ)"""
        processed_frame = self.preprocess_state(screen)
        
        # ì²« ë²ˆì§¸ í”„ë ˆì„ì¸ ê²½ìš° 4ë²ˆ ë³µì‚¬
        if len(self.frame_stack) == 0:
            for _ in range(4):
                self.frame_stack.append(processed_frame)
        else:
            self.frame_stack.append(processed_frame)
        
        # 4ê°œ í”„ë ˆì„ì„ ìŠ¤íƒìœ¼ë¡œ í•©ì¹¨
        state = np.stack(self.frame_stack, axis=0)
        return torch.FloatTensor(state).unsqueeze(0).to(self.device)
    
    def select_action(self, state, training=True):
        """í–‰ë™ ì„ íƒ (Îµ-greedy ì •ì±…)"""
        if training and random.random() < self.epsilon:
            # ëœë¤ íƒí—˜
            return random.randrange(self.num_actions)
        
        # Q-ê°’ ê¸°ë°˜ ì„ íƒ
        with torch.no_grad():
            q_values = self.q_network(state)
            return q_values.max(1)[1].item()
    
    def calculate_reward(self, game_state_before, game_state_after, action):
        """ë³´ìƒ ê³„ì‚°"""
        reward = 0
        
        # ì „íˆ¬ ê´€ë ¨ ë³´ìƒ
        if game_state_before.get("hp", 0) > game_state_after.get("hp", 0):
            reward -= 10  # HP ê°ì†Œ íŒ¨ë„í‹°
        
        if not game_state_before.get("is_battle") and game_state_after.get("is_battle"):
            reward += self.reward_system["find_enemy"]  # ì „íˆ¬ ë°œê²¬ ë³´ìƒ
        
        # íƒí—˜ ë³´ìƒ (ìƒˆë¡œìš´ ì˜ì—­)
        if self.is_new_area(game_state_after):
            reward += self.reward_system["explore_new_area"]
        
        # ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ íŒ¨ë„í‹°
        if action == 6:  # wait
            reward += self.reward_system["idle"]
        
        return reward
    
    def is_new_area(self, game_state):
        """ìƒˆë¡œìš´ ì˜ì—­ì¸ì§€ í™•ì¸ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” í™”ë©´ í•´ì‹œë‚˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©
        return random.random() < 0.1  # 10% í™•ë¥ ë¡œ ìƒˆ ì˜ì—­ìœ¼ë¡œ ê°„ì£¼
    
    def store_experience(self, state, action, reward, next_state, done):
        """ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì €ì¥"""
        self.replay_buffer.append((state, action, reward, next_state, done))
    
    def train(self):
        """ì‹ ê²½ë§ í•™ìŠµ"""
        if len(self.replay_buffer) < self.batch_size:
            return
        
        # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(self.replay_buffer, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.cat(states)
        next_states = torch.cat(next_states)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # í˜„ì¬ Qê°’
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Qê°’ (íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©)
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.learn_step += 1
        if self.learn_step % self.target_update_frequency == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        # Îµ ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, path):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'learn_step': self.learn_step
        }, path)
    
    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.learn_step = checkpoint['learn_step']


class EvolutionaryGameAI:
    """ì§„í™”í˜• ê²Œì„ AI (ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜)"""
    
    def __init__(self, population_size=20):
        self.population_size = population_size
        self.generation = 0
        
        # ê°œì²´êµ° (ê° ê°œì²´ëŠ” í–‰ë™ ì „ëµì„ ë‚˜íƒ€ëƒ„)
        self.population = self.initialize_population()
        
        # ì ì‘ë„ ì ìˆ˜
        self.fitness_scores = [0] * population_size
    
    def initialize_population(self):
        """ì´ˆê¸° ê°œì²´êµ° ìƒì„±"""
        population = []
        for _ in range(self.population_size):
            # ê° ê°œì²´ëŠ” ìƒí™©ë³„ í–‰ë™ ì „ëµì„ ê°€ì§
            individual = {
                "battle_strategy": self.random_strategy(),
                "exploration_strategy": self.random_strategy(),
                "survival_strategy": self.random_strategy()
            }
            population.append(individual)
        return population
    
    def random_strategy(self):
        """ëœë¤ ì „ëµ ìƒì„±"""
        return {
            "aggression": random.uniform(0, 1),
            "caution": random.uniform(0, 1),
            "exploration": random.uniform(0, 1),
            "patience": random.uniform(0, 1)
        }
    
    def select_action_evolutionary(self, individual, game_state):
        """ì§„í™”ëœ ê°œì²´ì˜ ì „ëµì— ë”°ë¥¸ í–‰ë™ ì„ íƒ"""
        if game_state.get("is_battle"):
            strategy = individual["battle_strategy"]
            if strategy["aggression"] > 0.7:
                return "attack"
            elif strategy["caution"] > 0.6:
                return "defend"
        else:
            strategy = individual["exploration_strategy"]
            if strategy["exploration"] > 0.5:
                return random.choice(["move_left", "move_right"])
        
        return "wait"
    
    def evaluate_fitness(self, individual, performance_data):
        """ê°œì²´ì˜ ì ì‘ë„ í‰ê°€"""
        fitness = 0
        
        # ìƒì¡´ ì‹œê°„
        fitness += performance_data.get("survival_time", 0) * 10
        
        # ì „íˆ¬ ìŠ¹ë¥ 
        win_rate = performance_data.get("battles_won", 0) / max(1, performance_data.get("total_battles", 1))
        fitness += win_rate * 100
        
        # íƒí—˜ ì ìˆ˜
        fitness += performance_data.get("areas_explored", 0) * 5
        
        return fitness
    
    def evolve_population(self):
        """ê°œì²´êµ° ì§„í™”"""
        # ì—˜ë¦¬íŠ¸ ì„ íƒ (ìƒìœ„ 20%)
        elite_count = self.population_size // 5
        elite_indices = np.argsort(self.fitness_scores)[-elite_count:]
        elite_population = [self.population[i] for i in elite_indices]
        
        # ìƒˆë¡œìš´ ê°œì²´êµ° ìƒì„±
        new_population = elite_population.copy()
        
        # êµë°°ì™€ ëŒì—°ë³€ì´
        while len(new_population) < self.population_size:
            parent1 = random.choice(elite_population)
            parent2 = random.choice(elite_population)
            
            child = self.crossover(parent1, parent2)
            child = self.mutate(child)
            
            new_population.append(child)
        
        self.population = new_population
        self.generation += 1
        
    def crossover(self, parent1, parent2):
        """êµë°° ì—°ì‚°"""
        child = {}
        for strategy_type in parent1.keys():
            child[strategy_type] = {}
            for param in parent1[strategy_type].keys():
                # ë¶€ëª¨ì˜ ìœ ì „ìë¥¼ ëœë¤í•˜ê²Œ ì„ íƒ
                if random.random() < 0.5:
                    child[strategy_type][param] = parent1[strategy_type][param]
                else:
                    child[strategy_type][param] = parent2[strategy_type][param]
        return child
    
    def mutate(self, individual, mutation_rate=0.1):
        """ëŒì—°ë³€ì´ ì—°ì‚°"""
        for strategy_type in individual.keys():
            for param in individual[strategy_type].keys():
                if random.random() < mutation_rate:
                    # 10% í™•ë¥ ë¡œ ëŒì—°ë³€ì´ ë°œìƒ
                    individual[strategy_type][param] = random.uniform(0, 1)
        return individual


# í†µí•© ì‚¬ìš© ì˜ˆì‹œ
def create_adaptive_ai():
    """ì ì‘í˜• AI ìƒì„±"""
    
    print("ğŸ§  ì ì‘í˜• ê²Œì„ AI ì´ˆê¸°í™”...")
    
    # ê°•í™”í•™ìŠµ AI
    rl_ai = ReinforcementLearningAI()
    print("âœ“ ê°•í™”í•™ìŠµ AI ì¤€ë¹„ ì™„ë£Œ")
    
    # ì§„í™” AI
    evo_ai = EvolutionaryGameAI()
    print("âœ“ ì§„í™”í˜• AI ì¤€ë¹„ ì™„ë£Œ")
    
    # LLM AI (ì„ íƒì )
    llm_available = False
    try:
        llm_ai = LLMGameAI()
        print("âœ“ LLM AI ì¤€ë¹„ ì™„ë£Œ")
        llm_available = True
    except:
        print("âš  LLM AI ë¹„í™œì„±í™” (ë¡œì»¬ ì„œë²„ í•„ìš”)")
    
    return {
        "reinforcement": rl_ai,
        "evolutionary": evo_ai,
        "llm": llm_ai if llm_available else None
    }

if __name__ == "__main__":
    print("ğŸš€ ììœ¨ í•™ìŠµ ê²Œì„ AI ì‹œìŠ¤í…œ")
    print("\nêµ¬í˜„ëœ ë°©ë²•ë“¤:")
    print("1. ê°•í™”í•™ìŠµ (DQN) - ê²½í—˜ì„ í†µí•œ í•™ìŠµ")
    print("2. ì§„í™” ì•Œê³ ë¦¬ì¦˜ - ì„¸ëŒ€ë³„ ì „ëµ ì§„í™”")  
    print("3. LLM ì—°ë™ - ìƒí™© íŒë‹¨ ë° ì¶”ë¡ ")
    
    adaptive_ai = create_adaptive_ai()
    print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ AI: {list(adaptive_ai.keys())}")