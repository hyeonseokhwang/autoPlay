"""
ğŸ§  ì™„ì „ ììœ¨ í•™ìŠµ AI (ì•ŒíŒŒê³  ì œë¡œ ìŠ¤íƒ€ì¼)
ì•„ë¬´ê²ƒë„ ê°€ë¥´ì¹˜ì§€ ì•Šê³  ìŠ¤ìŠ¤ë¡œ ê²Œì„ì„ í„°ë“í•˜ê²Œ í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import time
import json
import cv2
import numpy as np
from collections import deque, defaultdict
import requests
import hashlib
from datetime import datetime
import pickle
import os

class SelfLearningHeroAI:
    """ì™„ì „ ììœ¨ í•™ìŠµ ì˜ì›…ì „ì„¤4 AI - ì œë¡œ ì§€ì‹ì—ì„œ ì‹œì‘"""
    
    def __init__(self):
        from isolated_seeker import IsolatedDOSBoxSeeker
        
        self.base_seeker = IsolatedDOSBoxSeeker()
        
        # LLM ì„¤ì •
        self.llm_endpoint = "http://localhost:11434/api/generate"
        self.model = "qwen2.5-coder:7b"  # ì¶”ë¡  ëŠ¥ë ¥ì´ ì¢‹ì€ ëª¨ë¸
        
        # ì™„ì „ ììœ¨ í•™ìŠµì„ ìœ„í•œ ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge = {
            "screen_states": {},           # í™”ë©´ ìƒíƒœë³„ ê²½í—˜
            "action_consequences": {},     # í–‰ë™ â†’ ê²°ê³¼ ë§¤í•‘
            "successful_sequences": [],    # ì„±ê³µí•œ í–‰ë™ ì‹œí€€ìŠ¤
            "curiosity_targets": set(),    # íƒí—˜í•´ë³¼ ë§Œí•œ ê²ƒë“¤
            "learned_concepts": {},        # ìŠ¤ìŠ¤ë¡œ ë°œê²¬í•œ ê°œë…ë“¤
            "meta_strategies": []          # ê³ ìˆ˜ì¤€ ì „ëµë“¤
        }
        
        # ììœ¨ í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.exploration_rate = 0.8    # íƒí—˜ vs í™œìš©
        self.curiosity_threshold = 0.3 # ìƒˆë¡œìš´ ê²ƒì— ëŒ€í•œ ê´€ì‹¬ë„
        self.memory_size = 1000        # ê¸°ì–µí•  ê²½í—˜ ìˆ˜
        
        # ê²½í—˜ ë©”ëª¨ë¦¬ (ìë™ìœ¼ë¡œ íŒ¨í„´ì„ ì°¾ì•„ëƒ„)
        self.experiences = deque(maxlen=self.memory_size)
        self.screen_history = deque(maxlen=50)
        
        # ìì²´ ìƒì„± ë³´ìƒ ì‹œìŠ¤í…œ
        self.intrinsic_motivation = {
            "novelty_bonus": 10,        # ìƒˆë¡œìš´ ê²ƒ ë°œê²¬ ì‹œ ë³´ìƒ
            "progress_bonus": 5,        # ì§„ì „ì´ ìˆì„ ë•Œ ë³´ìƒ
            "consistency_bonus": 3,     # ì¼ê´€ëœ íŒ¨í„´ ë°œê²¬ ì‹œ ë³´ìƒ
            "exploration_bonus": 2      # íƒí—˜ ìì²´ì— ëŒ€í•œ ë³´ìƒ
        }
        
        print("ğŸ§  ì™„ì „ ììœ¨ í•™ìŠµ AI ì´ˆê¸°í™”...")
        print("ğŸ“š ê¸°ì¡´ ì§€ì‹: ì—†ìŒ (ì œë¡œ ì§€ì‹ ì‹œì‘)")
        print("ğŸ¯ ëª©í‘œ: ìŠ¤ìŠ¤ë¡œ ê²Œì„ ê·œì¹™ê³¼ ì „ëµ ë°œê²¬")
        
        self.load_previous_knowledge()
    
    def load_previous_knowledge(self):
        """ì´ì „ í•™ìŠµ ì„¸ì…˜ì˜ ì§€ì‹ ë¡œë“œ (ì„ íƒì )"""
        knowledge_file = "self_learned_knowledge.pkl"
        
        if os.path.exists(knowledge_file):
            try:
                with open(knowledge_file, 'rb') as f:
                    saved_knowledge = pickle.load(f)
                
                # ê¸°ì¡´ ì§€ì‹ê³¼ ë³‘í•© (ì™„ì „ ì¬ì‹œì‘ vs ê³„ì† í•™ìŠµ ì„ íƒ ê°€ëŠ¥)
                print("ğŸ” ì´ì „ í•™ìŠµ ë°ì´í„° ë°œê²¬!")
                print(f"   - ì•Œë ¤ì§„ í™”ë©´ ìƒíƒœ: {len(saved_knowledge.get('screen_states', {}))}")
                print(f"   - í•™ìŠµëœ ê°œë…: {len(saved_knowledge.get('learned_concepts', {}))}")
                print(f"   - ì„±ê³µ ì‹œí€€ìŠ¤: {len(saved_knowledge.get('successful_sequences', []))}")
                
                choice = input("ì´ì „ ì§€ì‹ ì‚¬ìš©? (y/n, ê¸°ë³¸ê°’ n): ").strip().lower()
                if choice == 'y':
                    self.knowledge = saved_knowledge
                    print("âœ… ì´ì „ ì§€ì‹ìœ¼ë¡œ ê³„ì† í•™ìŠµ")
                else:
                    print("ğŸ†• ì™„ì „ ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘")
                    
            except Exception as e:
                print(f"âš ï¸ ì´ì „ ì§€ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def analyze_screen_with_zero_knowledge(self, screen):
        """ì œë¡œ ì§€ì‹ ìƒíƒœì—ì„œ í™”ë©´ ë¶„ì„"""
        
        if screen is None:
            return {"type": "invalid", "features": {}}
        
        # í™”ë©´ì„ í•´ì‹œë¡œ ë³€í™˜í•˜ì—¬ ê³ ìœ  ì‹ë³„ì ìƒì„±
        screen_hash = self.hash_screen(screen)
        
        # LLMì—ê²Œ "ì²˜ìŒ ë³´ëŠ”" ê´€ì ì—ì„œ ë¶„ì„í•˜ê²Œ í•¨
        analysis_prompt = f"""
ë‹¹ì‹ ì€ ì²˜ìŒìœ¼ë¡œ ì´ ê²Œì„ í™”ë©´ì„ ë³´ëŠ” AIì…ë‹ˆë‹¤.
ì–´ë–¤ ê²Œì„ì¸ì§€, ì–´ë–¤ ê·œì¹™ì¸ì§€ ì „í˜€ ëª¨ë¦…ë‹ˆë‹¤.

í™”ë©´ì„ ë³´ê³  ë‹¤ìŒì„ ì¶”ë¡ í•´ì£¼ì„¸ìš”:
1. ì´ í™”ë©´ì—ì„œ ê°€ì¥ ëˆˆì— ë„ëŠ” ìš”ì†Œë“¤
2. ì›€ì§ì¼ ìˆ˜ ìˆëŠ” ê²ƒë“¤ (ìºë¦­í„°, ì»¤ì„œ ë“±)
3. ìˆ«ìë‚˜ ë°” í˜•íƒœì˜ ì •ë³´ë“¤
4. ë°˜ë³µë˜ëŠ” íŒ¨í„´ì´ë‚˜ êµ¬ì¡°ë“¤
5. ì´ì „ì— ë³¸ í™”ë©´ê³¼ì˜ ì°¨ì´ì  (ìˆë‹¤ë©´)

ìˆœìˆ˜í•˜ê²Œ ì‹œê°ì  ê´€ì°°ë§Œ í•˜ê³ , ê²Œì„ ìš©ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
JSON í˜•íƒœë¡œ ë‹µí•´ì£¼ì„¸ìš”:
{{
    "prominent_elements": ["ìš”ì†Œ1", "ìš”ì†Œ2"],
    "interactive_objects": ["ê°ì²´1", "ê°ì²´2"], 
    "numerical_info": ["ì •ë³´1", "ì •ë³´2"],
    "patterns": ["íŒ¨í„´1", "íŒ¨í„´2"],
    "screen_type": "ì¶”ì •_í™”ë©´_ìœ í˜•",
    "novelty_score": 0.8
}}
"""
        
        try:
            llm_response = self.call_llm(analysis_prompt)
            analysis = self.parse_llm_json(llm_response)
            
            # í™”ë©´ ìƒíƒœ ê¸°ë¡
            if screen_hash not in self.knowledge["screen_states"]:
                self.knowledge["screen_states"][screen_hash] = {
                    "first_seen": datetime.now().isoformat(),
                    "visit_count": 0,
                    "llm_analysis": analysis,
                    "discovered_actions": [],
                    "success_rate": 0.0
                }
                
                # ìƒˆë¡œìš´ í™”ë©´ ë°œê²¬ ë³´ìƒ
                novelty_reward = self.intrinsic_motivation["novelty_bonus"]
                print(f"ğŸ†• ìƒˆë¡œìš´ í™”ë©´ ìœ í˜• ë°œê²¬! (+{novelty_reward} ë³´ìƒ)")
            
            self.knowledge["screen_states"][screen_hash]["visit_count"] += 1
            return analysis
            
        except Exception as e:
            print(f"âš ï¸ í™”ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"type": "unknown", "features": {}}
    
    def hash_screen(self, screen):
        """í™”ë©´ì„ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜ (ìœ ì‚¬í•œ í™”ë©´ë¼ë¦¬ ê·¸ë£¹í•‘)"""
        
        # í™”ë©´ì„ ì‘ì€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ í•´ì‹œ ìƒì„±
        small_screen = cv2.resize(screen, (64, 48))
        gray = cv2.cvtColor(small_screen, cv2.COLOR_BGR2GRAY)
        
        # ê°„ë‹¨í•œ íŠ¹ì§• ê¸°ë°˜ í•´ì‹œ
        features = [
            np.mean(gray),              # í‰ê·  ë°ê¸°
            np.std(gray),               # ë°ê¸° í¸ì°¨
            len(np.unique(gray)),       # ìƒ‰ìƒ ë‹¤ì–‘ì„±
            cv2.Laplacian(gray, cv2.CV_64F).var()  # í…ìŠ¤ì²˜
        ]
        
        feature_str = "_".join(f"{f:.2f}" for f in features)
        return hashlib.md5(feature_str.encode()).hexdigest()[:8]
    
    def generate_curious_action(self, current_analysis):
        """í˜¸ê¸°ì‹¬ ê¸°ë°˜ í–‰ë™ ìƒì„±"""
        
        curiosity_prompt = f"""
ë‹¹ì‹ ì€ ì´ ê²Œì„ì„ ì²˜ìŒ í”Œë ˆì´í•˜ëŠ” AIì…ë‹ˆë‹¤.

í˜„ì¬ í™”ë©´ ë¶„ì„:
{json.dumps(current_analysis, ensure_ascii=False, indent=2)}

ë‹¤ìŒ ì¤‘ ì–´ë–¤ í–‰ë™ì„ ì‹œë„í•´ë³´ê³  ì‹¶ë‚˜ìš”? í˜¸ê¸°ì‹¬ê³¼ íƒí—˜ ì •ì‹ ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”:
0: ì™¼ìª½ ë°©í–¥í‚¤
1: ì˜¤ë¥¸ìª½ ë°©í–¥í‚¤  
2: ìœ„ìª½ ë°©í–¥í‚¤
3: ì•„ë˜ìª½ ë°©í–¥í‚¤
4: Enter/í™•ì¸í‚¤
5: Escape/ì·¨ì†Œí‚¤
6: ì ê¹ ê¸°ë‹¤ë¦¬ê¸°

ì„ íƒí•œ ì´ìœ ë„ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.

JSON í˜•íƒœë¡œ:
{{
    "action": 2,
    "reasoning": "ìœ„ìª½ì— ë­”ê°€ ìˆì–´ ë³´ì—¬ì„œ íƒí—˜í•´ë³´ê³  ì‹¶ìŒ",
    "curiosity_level": 0.8,
    "expected_outcome": "ìƒˆë¡œìš´ ì˜ì—­ì´ë‚˜ ì •ë³´ ë°œê²¬"
}}
"""
        
        try:
            response = self.call_llm(curiosity_prompt)
            decision = self.parse_llm_json(response)
            
            # í˜¸ê¸°ì‹¬ ìˆ˜ì¤€ì— ë”°ë¥¸ ë³´ìƒ
            curiosity = decision.get("curiosity_level", 0.5)
            if curiosity > self.curiosity_threshold:
                bonus = self.intrinsic_motivation["exploration_bonus"]
                print(f"ğŸ” ë†’ì€ í˜¸ê¸°ì‹¬ í–‰ë™! (+{bonus} ë³´ìƒ)")
            
            return decision
            
        except Exception as e:
            print(f"âš ï¸ í˜¸ê¸°ì‹¬ í–‰ë™ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ëœë¤ íƒí—˜
            return {
                "action": np.random.randint(0, 7),
                "reasoning": "ëœë¤ íƒí—˜ (LLM ì‹¤íŒ¨)",
                "curiosity_level": 0.3
            }
    
    def learn_from_consequence(self, before_screen, action, after_screen, meta_info):
        """í–‰ë™ì˜ ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ"""
        
        before_hash = self.hash_screen(before_screen)
        after_hash = self.hash_screen(after_screen)
        
        # ê²½í—˜ ê¸°ë¡
        experience = {
            "timestamp": time.time(),
            "before_state": before_hash,
            "action": action,
            "after_state": after_hash,
            "screen_changed": before_hash != after_hash,
            "meta_info": meta_info
        }
        
        self.experiences.append(experience)
        
        # í–‰ë™ ê²°ê³¼ ë¶„ì„ì„ LLMì—ê²Œ ë§¡ê¹€
        learning_prompt = f"""
í–‰ë™ì˜ ê²°ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

í–‰ë™: {action} ({self.get_action_name(action)})
í™”ë©´ ë³€í™”: {"ìˆìŒ" if before_hash != after_hash else "ì—†ìŒ"}
ì´ì „ í™”ë©´ ID: {before_hash}
ì´í›„ í™”ë©´ ID: {after_hash}

ì´ í–‰ë™ìœ¼ë¡œë¶€í„° ë¬´ì—‡ì„ ë°°ìš¸ ìˆ˜ ìˆë‚˜ìš”?

JSON í˜•íƒœë¡œ:
{{
    "learned_rule": "ë°°ìš´ ê·œì¹™ì´ë‚˜ íŒ¨í„´",
    "effectiveness": 0.7,
    "new_concept": "ìƒˆë¡œ ë°œê²¬í•œ ê°œë… (ìˆë‹¤ë©´)",
    "strategy_update": "ì „ëµ ì—…ë°ì´íŠ¸ ì‚¬í•­"
}}
"""
        
        try:
            learning_response = self.call_llm(learning_prompt)
            learning_result = self.parse_llm_json(learning_response)
            
            # ìƒˆë¡œìš´ ê°œë… ë°œê²¬ ì‹œ ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            if learning_result.get("new_concept"):
                concept = learning_result["new_concept"]
                if concept not in self.knowledge["learned_concepts"]:
                    self.knowledge["learned_concepts"][concept] = {
                        "discovered_at": datetime.now().isoformat(),
                        "confidence": learning_result.get("effectiveness", 0.5),
                        "examples": []
                    }
                    
                    bonus = self.intrinsic_motivation["progress_bonus"]
                    print(f"ğŸ’¡ ìƒˆë¡œìš´ ê°œë… ë°œê²¬: {concept} (+{bonus} ë³´ìƒ)")
            
            # í–‰ë™-ê²°ê³¼ ë§¤í•‘ ì—…ë°ì´íŠ¸
            state_action = f"{before_hash}_{action}"
            if state_action not in self.knowledge["action_consequences"]:
                self.knowledge["action_consequences"][state_action] = []
            
            self.knowledge["action_consequences"][state_action].append({
                "result_state": after_hash,
                "effectiveness": learning_result.get("effectiveness", 0.5),
                "timestamp": time.time()
            })
            
            return learning_result
            
        except Exception as e:
            print(f"âš ï¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return None
    
    def detect_progress_patterns(self):
        """ì§„ì „ íŒ¨í„´ ìë™ ê°ì§€"""
        
        if len(self.experiences) < 10:
            return
        
        recent_experiences = list(self.experiences)[-10:]
        
        # í™”ë©´ ë³€í™” íŒ¨í„´ ë¶„ì„
        screen_changes = [exp["screen_changed"] for exp in recent_experiences]
        change_rate = sum(screen_changes) / len(screen_changes)
        
        # ìƒˆë¡œìš´ í™”ë©´ ë°œê²¬ë¥ 
        unique_states = len(set(exp["after_state"] for exp in recent_experiences))
        novelty_rate = unique_states / len(recent_experiences)
        
        pattern_analysis_prompt = f"""
ìµœê·¼ 10ë²ˆì˜ í–‰ë™ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

í™”ë©´ ë³€í™”ìœ¨: {change_rate:.2f} (1.0ì´ ëª¨ë“  í–‰ë™ì—ì„œ í™”ë©´ì´ ë°”ë€œ)
ìƒˆë¡œìš´ í™”ë©´ ë¹„ìœ¨: {novelty_rate:.2f}

ì´ íŒ¨í„´ì—ì„œ ì–´ë–¤ ì§„ì „ì´ë‚˜ í•™ìŠµ ì‹ í˜¸ë¥¼ ë°œê²¬í•  ìˆ˜ ìˆë‚˜ìš”?

{{
    "progress_detected": true/false,
    "pattern_type": "íƒí—˜ì¤‘/ì •ì²´ì¤‘/í•™ìŠµì¤‘",
    "recommendation": "ë‹¤ìŒ í–‰ë™ ê¶Œì¥ì‚¬í•­"
}}
"""
        
        try:
            pattern_response = self.call_llm(pattern_analysis_prompt)
            pattern_result = self.parse_llm_json(pattern_response)
            
            if pattern_result.get("progress_detected"):
                bonus = self.intrinsic_motivation["consistency_bonus"]
                print(f"ğŸ“ˆ í•™ìŠµ ì§„ì „ ê°ì§€! ({pattern_result.get('pattern_type')}) (+{bonus} ë³´ìƒ)")
                
                # ë©”íƒ€ ì „ëµ ì—…ë°ì´íŠ¸
                self.knowledge["meta_strategies"].append({
                    "timestamp": time.time(),
                    "pattern": pattern_result.get("pattern_type"),
                    "recommendation": pattern_result.get("recommendation")
                })
            
        except Exception as e:
            print(f"âš ï¸ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def call_llm(self, prompt, timeout=10):
        """LLM í˜¸ì¶œ"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,  # ì°½ì˜ì  ì‚¬ê³ ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ê²Œ
                "top_p": 0.9
            }
        }
        
        response = requests.post(self.llm_endpoint, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            raise Exception(f"LLM API ì˜¤ë¥˜: {response.status_code}")
    
    def parse_llm_json(self, text):
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        try:
            # JSON ë¶€ë¶„ ì¶”ì¶œ
            start = text.find("{")
            end = text.rfind("}") + 1
            
            if start != -1 and end > start:
                json_text = text[start:end]
                return json.loads(json_text)
            else:
                return {}
        except:
            return {}
    
    def get_action_name(self, action_id):
        """í–‰ë™ IDë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        actions = {
            0: "ì™¼ìª½", 1: "ì˜¤ë¥¸ìª½", 2: "ìœ„ìª½", 3: "ì•„ë˜ìª½",
            4: "í™•ì¸", 5: "ì·¨ì†Œ", 6: "ëŒ€ê¸°"
        }
        return actions.get(action_id, "ì•Œìˆ˜ì—†ìŒ")
    
    def execute_action(self, action_id):
        """í–‰ë™ ì‹¤í–‰"""
        vk_keys = {
            0: self.base_seeker.VK_LEFT,
            1: self.base_seeker.VK_RIGHT,
            2: self.base_seeker.VK_UP,
            3: self.base_seeker.VK_DOWN,
            4: self.base_seeker.VK_RETURN,
            5: self.base_seeker.VK_ESCAPE,
            6: None  # ëŒ€ê¸°
        }
        
        vk_code = vk_keys.get(action_id)
        if vk_code is not None:
            return self.base_seeker.send_key_message(vk_code)
        
        return True
    
    def save_knowledge(self):
        """í•™ìŠµëœ ì§€ì‹ ì €ì¥"""
        knowledge_file = "self_learned_knowledge.pkl"
        
        try:
            with open(knowledge_file, 'wb') as f:
                pickle.dump(self.knowledge, f)
            
            print(f"ğŸ§  ì§€ì‹ ì €ì¥ ì™„ë£Œ: {knowledge_file}")
            print(f"   - ì•Œë ¤ì§„ í™”ë©´: {len(self.knowledge['screen_states'])}")
            print(f"   - í•™ìŠµëœ ê°œë…: {len(self.knowledge['learned_concepts'])}")
            print(f"   - ê²½í—˜ ìˆ˜: {len(self.experiences)}")
            
        except Exception as e:
            print(f"âŒ ì§€ì‹ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def autonomous_exploration(self, max_iterations=100):
        """ì™„ì „ ììœ¨ íƒí—˜ ë° í•™ìŠµ"""
        
        print("ğŸš€ ì™„ì „ ììœ¨ í•™ìŠµ ì‹œì‘!")
        print("ğŸ“‹ ê·œì¹™: AIê°€ ìŠ¤ìŠ¤ë¡œ ê²Œì„ì„ íƒí—˜í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤")
        print("ğŸ¯ ëª©í‘œ: ì•„ë¬´ê²ƒë„ ê°€ë¥´ì¹˜ì§€ ì•Šê³  ìŠ¤ìŠ¤ë¡œ í„°ë“í•˜ê²Œ í•˜ê¸°")
        print()
        
        if not self.base_seeker.find_dosbox_window():
            print("âŒ DOSBox ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        iteration = 0
        
        while iteration < max_iterations:
            try:
                print(f"\n--- íƒí—˜ #{iteration + 1} ---")
                
                # í˜„ì¬ í™”ë©´ ìº¡ì²˜
                current_screen = self.base_seeker.capture_dosbox_window()
                if current_screen is None:
                    print("âš ï¸ í™”ë©´ ìº¡ì²˜ ì‹¤íŒ¨")
                    continue
                
                # ì œë¡œ ì§€ì‹ í™”ë©´ ë¶„ì„
                screen_analysis = self.analyze_screen_with_zero_knowledge(current_screen)
                print(f"ğŸ” í™”ë©´ ë¶„ì„: {screen_analysis.get('screen_type', 'ì•Œìˆ˜ì—†ìŒ')}")
                
                # í˜¸ê¸°ì‹¬ ê¸°ë°˜ í–‰ë™ ê²°ì •
                action_decision = self.generate_curious_action(screen_analysis)
                action = action_decision["action"]
                reasoning = action_decision.get("reasoning", "")
                
                print(f"ğŸ¤” ì„ íƒí•œ í–‰ë™: {self.get_action_name(action)} - {reasoning}")
                
                # í–‰ë™ ì‹¤í–‰
                self.execute_action(action)
                
                # ì ì‹œ ëŒ€ê¸° í›„ ê²°ê³¼ ê´€ì°°
                time.sleep(1)
                
                # ê²°ê³¼ í™”ë©´ ìº¡ì²˜
                result_screen = self.base_seeker.capture_dosbox_window()
                if result_screen is not None:
                    # ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ
                    meta_info = {
                        "iteration": iteration,
                        "reasoning": reasoning,
                        "curiosity_level": action_decision.get("curiosity_level", 0.5)
                    }
                    
                    learning_result = self.learn_from_consequence(
                        current_screen, action, result_screen, meta_info
                    )
                    
                    if learning_result:
                        learned_rule = learning_result.get("learned_rule", "")
                        if learned_rule:
                            print(f"ğŸ’¡ í•™ìŠµ: {learned_rule}")
                
                # ì£¼ê¸°ì  íŒ¨í„´ ë¶„ì„
                if iteration % 10 == 0 and iteration > 0:
                    print(f"\nğŸ“Š {iteration}ë²ˆ íƒí—˜ í›„ íŒ¨í„´ ë¶„ì„...")
                    self.detect_progress_patterns()
                    self.print_learning_summary()
                
                # ì£¼ê¸°ì  ì €ì¥
                if iteration % 25 == 0 and iteration > 0:
                    self.save_knowledge()
                
                iteration += 1
                
                # íƒí—˜ë¥  ì¡°ì • (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ í™œìš© ì¦ê°€)
                self.exploration_rate = max(0.1, self.exploration_rate * 0.995)
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                iteration += 1
        
        # ìµœì¢… ì €ì¥
        self.save_knowledge()
        print("\nğŸ‰ ììœ¨ í•™ìŠµ ì™„ë£Œ!")
        self.print_final_report()
    
    def print_learning_summary(self):
        """ì¤‘ê°„ í•™ìŠµ ìš”ì•½"""
        print("\nğŸ“š í˜„ì¬ í•™ìŠµ ìƒí™©:")
        print(f"   ì•Œë ¤ì§„ í™”ë©´ ìœ í˜•: {len(self.knowledge['screen_states'])}")
        print(f"   ë°œê²¬í•œ ê°œë…: {len(self.knowledge['learned_concepts'])}")
        print(f"   ì¶•ì ëœ ê²½í—˜: {len(self.experiences)}")
        
        if self.knowledge['learned_concepts']:
            print("   ìµœê·¼ ë°œê²¬í•œ ê°œë…ë“¤:")
            for concept, info in list(self.knowledge['learned_concepts'].items())[-3:]:
                confidence = info.get('confidence', 0)
                print(f"     - {concept} (ì‹ ë¢°ë„: {confidence:.2f})")
    
    def print_final_report(self):
        """ìµœì¢… í•™ìŠµ ë³´ê³ ì„œ"""
        print("\n" + "="*50)
        print("ğŸ§  ì™„ì „ ììœ¨ í•™ìŠµ ìµœì¢… ë³´ê³ ì„œ")
        print("="*50)
        
        print(f"ğŸ“Š í†µê³„:")
        print(f"   - ë°œê²¬í•œ í™”ë©´ ìœ í˜•: {len(self.knowledge['screen_states'])}")
        print(f"   - í•™ìŠµí•œ ê°œë…: {len(self.knowledge['learned_concepts'])}")
        print(f"   - ëˆ„ì  ê²½í—˜: {len(self.experiences)}")
        print(f"   - ê°œë°œí•œ ì „ëµ: {len(self.knowledge['meta_strategies'])}")
        
        print(f"\nğŸ’¡ ë°œê²¬í•œ ì£¼ìš” ê°œë…ë“¤:")
        for concept, info in self.knowledge['learned_concepts'].items():
            confidence = info.get('confidence', 0)
            discovered_time = info.get('discovered_at', 'ì•Œìˆ˜ì—†ìŒ')
            print(f"   - {concept} (ì‹ ë¢°ë„: {confidence:.2f}) - {discovered_time[:10]}")
        
        print(f"\nğŸ¯ ê°œë°œí•œ ë©”íƒ€ ì „ëµ:")
        for strategy in self.knowledge['meta_strategies'][-5:]:  # ìµœê·¼ 5ê°œ
            pattern = strategy.get('pattern', 'ì•Œìˆ˜ì—†ìŒ')
            recommendation = strategy.get('recommendation', '')
            print(f"   - {pattern}: {recommendation}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ§  ì•ŒíŒŒê³  ì œë¡œ ìŠ¤íƒ€ì¼ ì™„ì „ ììœ¨ í•™ìŠµ AI")
    print("ğŸ¯ ëª©í‘œ: ì•„ë¬´ê²ƒë„ ê°€ë¥´ì¹˜ì§€ ì•Šê³  ìŠ¤ìŠ¤ë¡œ ê²Œì„ í„°ë“í•˜ê¸°")
    print()
    
    print("ğŸ“‹ ì¤€ë¹„ì‚¬í•­:")
    print("1. DOSBoxì—ì„œ ì˜ì›…ì „ì„¤4 ì‹¤í–‰")
    print("2. LLM ëª¨ë¸ ì‹¤í–‰ (qwen2.5-coder:7b ê¶Œì¥)")
    print("3. ì‹œê°„ ì—¬ìœ  (í•™ìŠµì—ëŠ” ì‹œê°„ì´ í•„ìš”)")
    print()
    
    print("âš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("- ì²˜ìŒì—ëŠ” ë¬´ì‘ìœ„ë¡œ í–‰ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("- ì ì§„ì ìœ¼ë¡œ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤")
    print("- í•™ìŠµëœ ì§€ì‹ì€ ìë™ ì €ì¥ë©ë‹ˆë‹¤")
    print()
    
    try:
        iterations = input("íƒí—˜ íšŸìˆ˜ (ê¸°ë³¸ê°’ 100): ").strip()
        max_iterations = int(iterations) if iterations else 100
        
        print(f"\nğŸš€ {max_iterations}ë²ˆì˜ ììœ¨ íƒí—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("ì‹œì‘í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        input()
        
        # ììœ¨ í•™ìŠµ AI ì‹¤í–‰
        ai = SelfLearningHeroAI()
        ai.autonomous_exploration(max_iterations=max_iterations)
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ììœ¨ í•™ìŠµ AI ê°œë°œ í™”ì´íŒ…!")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()