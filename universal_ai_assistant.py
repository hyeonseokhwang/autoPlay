"""
ğŸ¤– ë²”ìš© ììœ¨í•™ìŠµ AI ë¹„ì„œ
ê²Œì„ í”Œë ˆì´ â†’ ì¸í„°ë„· ì„œí•‘ â†’ ì¼ë°˜ ì—…ë¬´ ë„ìš°ë¯¸ë¡œ ì§„í™”í•˜ëŠ” AI
"""

import requests
import json
import time
import sqlite3
import asyncio
import aiohttp
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin
import re
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import schedule
import threading

class UniversalLearningAssistant:
    """ë²”ìš© ììœ¨í•™ìŠµ AI ë¹„ì„œ"""
    
    def __init__(self):
        # LLM ì„¤ì •
        self.llm_endpoint = "http://localhost:11434/api/generate"
        self.model = "qwen2.5-coder:7b"
        
        # ì„ë² ë”© ëª¨ë¸ (GPU ê°€ì†)
        self.embedding_model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
            device='cuda' if self.check_gpu_available() else 'cpu'
        )
        
        # ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤
        self.init_knowledge_database()
        
        # í•™ìŠµ ì˜ì—­ë“¤
        self.learning_domains = {
            "game_strategy": "ê²Œì„ ê³µëµ ë° ì „ëµ",
            "web_research": "ì¸í„°ë„· ì •ë³´ ìˆ˜ì§‘", 
            "task_management": "ì—…ë¬´ ê´€ë¦¬",
            "knowledge_synthesis": "ì •ë³´ ì¢…í•© ë¶„ì„",
            "conversation": "ëŒ€í™” ë° ì†Œí†µ",
            "problem_solving": "ë¬¸ì œ í•´ê²°"
        }
        
        # ììœ¨ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬
        self.learning_schedule = {}
        self.setup_autonomous_learning()
        
        print("ğŸ¤– ë²”ìš© ììœ¨í•™ìŠµ AI ë¹„ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸: {'GPU' if self.embedding_model.device.type == 'cuda' else 'CPU'} ì‚¬ìš©")
    
    def check_gpu_available(self):
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def init_knowledge_database(self):
        """ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.conn = sqlite3.connect('universal_ai_knowledge.db', check_same_thread=False)
        cursor = self.conn.cursor()
        
        # ì›¹ ì •ë³´ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS web_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                domain TEXT,
                url TEXT,
                title TEXT,
                content TEXT,
                summary TEXT,
                embedding_vector TEXT,
                relevance_score REAL,
                last_updated TIMESTAMP,
                source_type TEXT
            )
        """)
        
        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS conversation_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_input TEXT,
                ai_response TEXT,
                context_embedding TEXT,
                satisfaction_score REAL,
                timestamp TIMESTAMP,
                domain TEXT
            )
        """)
        
        # í•™ìŠµ ì§„í–‰ë„ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS learning_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                domain TEXT,
                skill_name TEXT,
                proficiency_level REAL,
                learning_count INTEGER,
                last_practice TIMESTAMP,
                next_review TIMESTAMP
            )
        """)
        
        # ì‘ì—… ê¸°ë¡ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS task_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_description TEXT,
                execution_method TEXT,
                success_rate REAL,
                time_taken REAL,
                learned_optimization TEXT,
                timestamp TIMESTAMP
            )
        """)
        
        self.conn.commit()
    
    async def autonomous_web_research(self, topic, depth=3):
        """ììœ¨ì  ì›¹ ë¦¬ì„œì¹˜"""
        print(f"ğŸ” '{topic}' ì£¼ì œë¡œ ììœ¨ ì›¹ ë¦¬ì„œì¹˜ ì‹œì‘...")
        
        research_results = {
            'topic': topic,
            'sources': [],
            'key_insights': [],
            'related_topics': [],
            'confidence_score': 0.0
        }
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ë° ë‹¤ê°í™”
        search_queries = await self.generate_search_queries(topic)
        
        async with aiohttp.ClientSession() as session:
            for query in search_queries[:depth]:
                try:
                    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì—”ì§„ í™œìš©
                    results = await self.search_multiple_sources(session, query)
                    
                    for result in results:
                        # ì½˜í…ì¸  ë¶„ì„ ë° ìš”ì•½
                        analyzed_content = await self.analyze_web_content(
                            session, result['url'], result['title']
                        )
                        
                        if analyzed_content:
                            research_results['sources'].append(analyzed_content)
                            
                            # ì§€ì‹ ë² ì´ìŠ¤ì— ì €ì¥
                            self.store_web_knowledge(
                                domain=topic,
                                url=result['url'],
                                title=result['title'],
                                content=analyzed_content['content'],
                                summary=analyzed_content['summary']
                            )
                
                except Exception as e:
                    print(f"âš ï¸ ë¦¬ì„œì¹˜ ì˜¤ë¥˜: {e}")
        
        # ìˆ˜ì§‘ëœ ì •ë³´ ì¢…í•© ë¶„ì„
        research_results['key_insights'] = await self.synthesize_insights(research_results['sources'])
        research_results['related_topics'] = await self.find_related_topics(topic, research_results['sources'])
        research_results['confidence_score'] = self.calculate_research_confidence(research_results)
        
        print(f"âœ… ì›¹ ë¦¬ì„œì¹˜ ì™„ë£Œ: {len(research_results['sources'])}ê°œ ì†ŒìŠ¤ ìˆ˜ì§‘")
        return research_results
    
    async def generate_search_queries(self, topic):
        """ì£¼ì œì— ëŒ€í•œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        
        prompt = f"""
ì£¼ì œ: {topic}

ì´ ì£¼ì œì— ëŒ€í•´ í¬ê´„ì ìœ¼ë¡œ ì—°êµ¬í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ 5ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì„¸ë¶€ ì˜ì—­ì„ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.

JSON í˜•íƒœë¡œ:
{{
    "queries": [
        "ê¸°ë³¸ ì¿¼ë¦¬ 1",
        "ì‹¬í™” ì¿¼ë¦¬ 2", 
        "ì‹¤ìš©ì  ì¿¼ë¦¬ 3",
        "ì „ë¬¸ê°€ ê´€ì  ì¿¼ë¦¬ 4",
        "ìµœì‹  ë™í–¥ ì¿¼ë¦¬ 5"
    ]
}}
"""
        
        try:
            response = await self.call_llm_async(prompt)
            parsed = self.parse_llm_json(response)
            return parsed.get('queries', [topic])
        except:
            # ê¸°ë³¸ ì¿¼ë¦¬ë“¤
            return [
                topic,
                f"{topic} ê°€ì´ë“œ",
                f"{topic} íŒ",
                f"{topic} ì „ëµ",
                f"{topic} ìµœì‹ "
            ]
    
    async def search_multiple_sources(self, session, query):
        """ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰"""
        
        search_engines = [
            f"https://www.google.com/search?q={quote(query)}",
            f"https://www.bing.com/search?q={quote(query)}",
            f"https://duckduckgo.com/?q={quote(query)}"
        ]
        
        results = []
        
        for engine_url in search_engines[:1]:  # ì¼ë‹¨ êµ¬ê¸€ë§Œ
            try:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê²€ìƒ‰ API ì‚¬ìš© ê¶Œì¥
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
                mock_results = [
                    {
                        'url': f"https://example.com/article1?q={quote(query)}",
                        'title': f"{query} ê´€ë ¨ ì •ë³´ 1",
                        'snippet': f"{query}ì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´"
                    },
                    {
                        'url': f"https://example.com/article2?q={quote(query)}",
                        'title': f"{query} ìƒì„¸ ê°€ì´ë“œ",
                        'snippet': f"{query} ì‹¬í™” í•™ìŠµ ìë£Œ"
                    }
                ]
                results.extend(mock_results)
                
            except Exception as e:
                print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    async def analyze_web_content(self, session, url, title):
        """ì›¹ ì½˜í…ì¸  ë¶„ì„ ë° ìš”ì•½"""
        
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›¹í˜ì´ì§€ í¬ë¡¤ë§
            # ì—¬ê¸°ì„œëŠ” ëª¨ì˜ ë°ì´í„°
            mock_content = f"""
            {title}ì— ëŒ€í•œ ìƒì„¸í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
            
            ì£¼ìš” í¬ì¸íŠ¸:
            1. ê¸°ë³¸ ê°œë…ê³¼ ì •ì˜
            2. ì‹¤ìš©ì ì¸ í™œìš© ë°©ë²•
            3. ì „ë¬¸ê°€ íŒê³¼ ì¡°ì–¸
            4. ìµœì‹  ë™í–¥ ë° ì—…ë°ì´íŠ¸
            
            ì´ ì •ë³´ëŠ” {url}ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.
            """
            
            # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ ì›¹ ì½˜í…ì¸ ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
URL: {url}
ë‚´ìš©: {mock_content}

í•µì‹¬ ì •ë³´ë§Œ 3-4ì¤„ë¡œ ìš”ì•½:
"""
            
            summary = await self.call_llm_async(summary_prompt)
            
            return {
                'url': url,
                'title': title,
                'content': mock_content,
                'summary': summary.strip(),
                'word_count': len(mock_content.split()),
                'relevance': 0.8  # ê´€ë ¨ì„± ì ìˆ˜
            }
            
        except Exception as e:
            print(f"ì½˜í…ì¸  ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def store_web_knowledge(self, domain, url, title, content, summary):
        """ì›¹ ì§€ì‹ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        
        # ì„ë² ë”© ìƒì„±
        combined_text = f"{title} {summary} {content}"
        embedding = self.embedding_model.encode(combined_text)
        embedding_json = json.dumps(embedding.tolist())
        
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO web_knowledge 
            (domain, url, title, content, summary, embedding_vector, 
             relevance_score, last_updated, source_type)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            domain, url, title, content, summary, embedding_json,
            0.8, datetime.now().isoformat(), 'web_research'
        ))
        
        self.conn.commit()
    
    async def synthesize_insights(self, sources):
        """ìˆ˜ì§‘ëœ ì†ŒìŠ¤ë“¤ë¡œë¶€í„° í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        if not sources:
            return []
        
        combined_summaries = "\n\n".join([s['summary'] for s in sources])
        
        synthesis_prompt = f"""
ë‹¤ìŒ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{combined_summaries}

JSON í˜•íƒœë¡œ:
{{
    "insights": [
        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1",
        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 2",
        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3",
        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 4", 
        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 5"
    ]
}}
"""
        
        try:
            response = await self.call_llm_async(synthesis_prompt)
            parsed = self.parse_llm_json(response)
            return parsed.get('insights', [])
        except:
            return ["ì •ë³´ ì¢…í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ"]
    
    async def find_related_topics(self, main_topic, sources):
        """ê´€ë ¨ ì£¼ì œ ë°œê²¬"""
        
        content_text = " ".join([s['content'] for s in sources])
        
        related_prompt = f"""
ì£¼ìš” ì£¼ì œ: {main_topic}

ë‹¤ìŒ ë‚´ìš©ì—ì„œ ê´€ë ¨ëœ í•˜ìœ„ ì£¼ì œë‚˜ ì—°ê´€ ì£¼ì œë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”:

{content_text[:1000]}...

JSON í˜•íƒœë¡œ:
{{
    "related_topics": [
        "ê´€ë ¨ ì£¼ì œ 1",
        "ê´€ë ¨ ì£¼ì œ 2",
        "ê´€ë ¨ ì£¼ì œ 3"
    ]
}}
"""
        
        try:
            response = await self.call_llm_async(related_prompt)
            parsed = self.parse_llm_json(response)
            return parsed.get('related_topics', [])
        except:
            return []
    
    def calculate_research_confidence(self, research_results):
        """ë¦¬ì„œì¹˜ ê²°ê³¼ì˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        source_count = len(research_results['sources'])
        avg_relevance = np.mean([s.get('relevance', 0.5) for s in research_results['sources']])
        insight_count = len(research_results['key_insights'])
        
        confidence = (
            min(source_count / 5, 1.0) * 0.4 +  # ì†ŒìŠ¤ ë‹¤ì–‘ì„±
            avg_relevance * 0.4 +               # ê´€ë ¨ì„±
            min(insight_count / 5, 1.0) * 0.2   # ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ
        )
        
        return round(confidence, 2)
    
    async def intelligent_conversation(self, user_input, context_history=None):
        """ì§€ëŠ¥ì  ëŒ€í™” ì²˜ë¦¬"""
        
        print(f"ğŸ’¬ ì‚¬ìš©ì: {user_input}")
        
        # ì‚¬ìš©ì ì…ë ¥ ì˜ë„ ë¶„ì„
        intent = await self.analyze_user_intent(user_input)
        
        # ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        relevant_knowledge = self.search_relevant_knowledge(user_input)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = {
            'user_input': user_input,
            'intent': intent,
            'relevant_knowledge': relevant_knowledge,
            'conversation_history': context_history or []
        }
        
        # AI ì‘ë‹µ ìƒì„±
        response = await self.generate_contextual_response(context)
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.store_conversation(user_input, response, intent['domain'])
        
        print(f"ğŸ¤– AI: {response}")
        return response
    
    async def analyze_user_intent(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ ì˜ë„ ë¶„ì„"""
        
        intent_prompt = f"""
ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”:

ì…ë ¥: "{user_input}"

JSON í˜•íƒœë¡œ:
{{
    "intent_type": "question/request/conversation/task",
    "domain": "game_strategy/web_research/task_management/general",
    "urgency": "low/medium/high",
    "requires_research": true/false,
    "specific_action": "êµ¬ì²´ì ì¸ í–‰ë™ (ìˆë‹¤ë©´)"
}}
"""
        
        try:
            response = await self.call_llm_async(intent_prompt)
            return self.parse_llm_json(response)
        except:
            return {
                "intent_type": "conversation",
                "domain": "general", 
                "urgency": "medium",
                "requires_research": False
            }
    
    def search_relevant_knowledge(self, query, top_k=3):
        """ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜)"""
        
        query_embedding = self.embedding_model.encode(query)
        
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT title, summary, content, relevance_score, source_type 
            FROM web_knowledge 
            ORDER BY last_updated DESC LIMIT 50
        """)
        
        knowledge_items = cursor.fetchall()
        
        if not knowledge_items:
            return []
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for item in knowledge_items:
            try:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì €ì¥ëœ ì„ë² ë”© ì‚¬ìš©
                item_text = f"{item[0]} {item[1]}"
                item_embedding = self.embedding_model.encode(item_text)
                
                similarity = cosine_similarity([query_embedding], [item_embedding])[0][0]
                
                similarities.append({
                    'title': item[0],
                    'summary': item[1], 
                    'content': item[2][:500],  # ì²« 500ìë§Œ
                    'similarity': similarity,
                    'source_type': item[4]
                })
            except:
                continue
        
        # ìœ ì‚¬ë„ ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]
    
    async def generate_contextual_response(self, context):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        
        user_input = context['user_input']
        intent = context['intent']
        relevant_knowledge = context['relevant_knowledge']
        
        # ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        knowledge_context = ""
        if relevant_knowledge:
            knowledge_context = "\nì°¸ê³  ì •ë³´:\n"
            for i, item in enumerate(relevant_knowledge, 1):
                knowledge_context += f"{i}. {item['title']}: {item['summary']}\n"
        
        response_prompt = f"""
ë‹¹ì‹ ì€ ì§€ëŠ¥ì ì¸ AI ë¹„ì„œì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì…ë ¥: {user_input}
ì˜ë„ ë¶„ì„: {json.dumps(intent, ensure_ascii=False)}

{knowledge_context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€
- í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ ì œì•ˆ
- ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤

ì‘ë‹µ:
"""
        
        try:
            response = await self.call_llm_async(response_prompt)
            
            # ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•œ ê²½ìš° ìë™ íŠ¸ë¦¬ê±°
            if intent.get('requires_research') and intent.get('domain') != 'general':
                research_topic = self.extract_research_topic(user_input)
                if research_topic:
                    print(f"ğŸ” '{research_topic}'ì— ëŒ€í•œ ìë™ ë¦¬ì„œì¹˜ ì‹œì‘...")
                    asyncio.create_task(self.autonomous_web_research(research_topic))
            
            return response.strip()
            
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def extract_research_topic(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ë¦¬ì„œì¹˜ ì£¼ì œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
        keywords = re.findall(r'\b[ê°€-í£]{2,}\b', user_input)
        return " ".join(keywords[:3]) if keywords else None
    
    def store_conversation(self, user_input, ai_response, domain):
        """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
        
        context_text = f"User: {user_input} AI: {ai_response}"
        context_embedding = self.embedding_model.encode(context_text)
        embedding_json = json.dumps(context_embedding.tolist())
        
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO conversation_history 
            (user_input, ai_response, context_embedding, satisfaction_score, timestamp, domain)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            user_input, ai_response, embedding_json, 
            0.8, datetime.now().isoformat(), domain
        ))
        
        self.conn.commit()
    
    def setup_autonomous_learning(self):
        """ììœ¨ í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        
        # ë§¤ì¼ ììœ¨ í•™ìŠµ ì„¸ì…˜
        schedule.every().day.at("09:00").do(self.daily_learning_session)
        schedule.every().day.at("18:00").do(self.evening_research_session)
        
        # ì£¼ê¸°ì  ì§€ì‹ ì •ë¦¬
        schedule.every().week.do(self.weekly_knowledge_synthesis)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        
        print("ğŸ“… ììœ¨ í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")
    
    async def daily_learning_session(self):
        """ì¼ì¼ ììœ¨ í•™ìŠµ ì„¸ì…˜"""
        print("ğŸ“š ì¼ì¼ ììœ¨ í•™ìŠµ ì„¸ì…˜ ì‹œì‘...")
        
        # ìµœê·¼ ëŒ€í™”ì—ì„œ ìì£¼ ì–¸ê¸‰ëœ ì£¼ì œ ë¶„ì„
        trending_topics = self.analyze_trending_topics()
        
        for topic in trending_topics[:2]:  # ìƒìœ„ 2ê°œ ì£¼ì œ
            await self.autonomous_web_research(topic, depth=2)
        
        print("âœ… ì¼ì¼ í•™ìŠµ ì™„ë£Œ")
    
    async def evening_research_session(self):
        """ì €ë… ë¦¬ì„œì¹˜ ì„¸ì…˜"""
        print("ğŸŒ™ ì €ë… ì‹¬í™” ë¦¬ì„œì¹˜ ì„¸ì…˜...")
        
        # ì§€ì‹ ê²©ì°¨ ë¶„ì„ ë° ë³´ì™„
        knowledge_gaps = self.identify_knowledge_gaps()
        
        for gap in knowledge_gaps[:1]:  # ê°€ì¥ í° ê²©ì°¨ 1ê°œ
            await self.autonomous_web_research(gap, depth=3)
    
    def analyze_trending_topics(self):
        """ìµœê·¼ ëŒ€í™”ì—ì„œ íŠ¸ë Œë”© ì£¼ì œ ë¶„ì„"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT user_input FROM conversation_history 
            WHERE timestamp > datetime('now', '-7 days')
            ORDER BY timestamp DESC LIMIT 20
        """)
        
        recent_inputs = [row[0] for row in cursor.fetchall()]
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        all_text = " ".join(recent_inputs)
        keywords = re.findall(r'\b[ê°€-í£]{2,}\b', all_text)
        
        from collections import Counter
        keyword_counts = Counter(keywords)
        
        return [word for word, count in keyword_counts.most_common(5)]
    
    def identify_knowledge_gaps(self):
        """ì§€ì‹ ê²©ì°¨ ì‹ë³„"""
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”
        potential_gaps = [
            "ìµœì‹  ê¸°ìˆ  ë™í–¥",
            "ì—…ë¬´ íš¨ìœ¨ì„± í–¥ìƒ",
            "ì°½ì‘ ë° ê¸€ì“°ê¸°",
            "ë°ì´í„° ë¶„ì„ ë°©ë²•"
        ]
        
        return potential_gaps[:2]
    
    async def weekly_knowledge_synthesis(self):
        """ì£¼ê°„ ì§€ì‹ ì¢…í•©"""
        print("ğŸ“Š ì£¼ê°„ ì§€ì‹ ì¢…í•© ì‘ì—…...")
        
        # ìˆ˜ì§‘ëœ ì§€ì‹ í†µí•© ë° ì •ë¦¬
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT domain, COUNT(*) as knowledge_count
            FROM web_knowledge 
            WHERE last_updated > datetime('now', '-7 days')
            GROUP BY domain
        """)
        
        weekly_stats = cursor.fetchall()
        
        print("ì£¼ê°„ í•™ìŠµ í†µê³„:")
        for domain, count in weekly_stats:
            print(f"  - {domain}: {count}ê°œ ì§€ì‹ í•­ëª© ìˆ˜ì§‘")
    
    async def call_llm_async(self, prompt, timeout=15):
        """ë¹„ë™ê¸° LLM í˜¸ì¶œ"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
        
        # aiohttp ì‚¬ìš© ì‹œ ë¹„ë™ê¸° ì²˜ë¦¬
        async with aiohttp.ClientSession() as session:
            async with session.post(self.llm_endpoint, json=payload, timeout=timeout) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("response", "")
                else:
                    raise Exception(f"LLM API ì˜¤ë¥˜: {response.status}")
    
    def parse_llm_json(self, text):
        """LLM ì‘ë‹µ JSON íŒŒì‹±"""
        try:
            start = text.find("{")
            end = text.rfind("}") + 1
            
            if start != -1 and end > start:
                json_text = text[start:end]
                return json.loads(json_text)
            return {}
        except:
            return {}
    
    async def run_interactive_session(self):
        """ëŒ€í™”í˜• ì„¸ì…˜ ì‹¤í–‰"""
        print("\nğŸ¤– ë²”ìš© AI ë¹„ì„œì™€ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("ğŸ’¡ 'ì›¹ì„œì¹˜ [ì£¼ì œ]'ë¡œ ìë™ ë¦¬ì„œì¹˜ ìš”ì²­ ê°€ëŠ¥")
        print("ğŸ’¡ 'exit'ë¡œ ì¢…ë£Œ")
        
        conversation_history = []
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if user_input.lower() == 'exit':
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¦ê±°ì› ì–´ìš”!")
                    break
                
                if user_input.startswith('ì›¹ì„œì¹˜ '):
                    # ìˆ˜ë™ ì›¹ ë¦¬ì„œì¹˜ íŠ¸ë¦¬ê±°
                    topic = user_input[4:].strip()
                    research_result = await self.autonomous_web_research(topic)
                    
                    summary = f"'{topic}' ë¦¬ì„œì¹˜ ì™„ë£Œ!\n"
                    summary += f"ğŸ“š {len(research_result['sources'])}ê°œ ì†ŒìŠ¤ ìˆ˜ì§‘\n"
                    summary += f"ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {len(research_result['key_insights'])}ê°œ\n"
                    summary += f"ğŸ¯ ì‹ ë¢°ë„: {research_result['confidence_score']}\n"
                    
                    if research_result['key_insights']:
                        summary += "\nì£¼ìš” ë°œê²¬ì‚¬í•­:\n"
                        for i, insight in enumerate(research_result['key_insights'][:3], 1):
                            summary += f"{i}. {insight}\n"
                    
                    print(f"\nğŸ¤– AI: {summary}")
                else:
                    # ì¼ë°˜ ëŒ€í™”
                    response = await self.intelligent_conversation(user_input, conversation_history)
                    conversation_history.append({
                        'user': user_input,
                        'ai': response,
                        'timestamp': datetime.now()
                    })
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
                    if len(conversation_history) > 10:
                        conversation_history = conversation_history[-10:]
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Ctrl+Cë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ë²”ìš© ììœ¨í•™ìŠµ AI ë¹„ì„œ")
    print("ğŸ§  ê²Œì„ AI â†’ ì›¹ ë¦¬ì„œì¹˜ â†’ ê°œì¸ ë¹„ì„œë¡œ ì§„í™”")
    print("ğŸ”¥ GPU ê°€ì† ë²¡í„° ê²€ìƒ‰ + ììœ¨ í•™ìŠµ")
    print()
    
    # AI ì´ˆê¸°í™”
    assistant = UniversalLearningAssistant()
    
    print("\në©”ë‰´:")
    print("1. ëŒ€í™”í˜• ë¹„ì„œ ëª¨ë“œ")
    print("2. ììœ¨ ì›¹ ë¦¬ì„œì¹˜ í…ŒìŠ¤íŠ¸")
    print("3. ì§€ì‹ í˜„í™© í™•ì¸")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    if choice == "1":
        await assistant.run_interactive_session()
        
    elif choice == "2":
        topic = input("ë¦¬ì„œì¹˜ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if topic:
            result = await assistant.autonomous_web_research(topic)
            print(f"\nğŸ“Š ë¦¬ì„œì¹˜ ê²°ê³¼:")
            print(f"   ì†ŒìŠ¤: {len(result['sources'])}ê°œ")
            print(f"   ì¸ì‚¬ì´íŠ¸: {len(result['key_insights'])}ê°œ")
            print(f"   ì‹ ë¢°ë„: {result['confidence_score']}")
    
    elif choice == "3":
        cursor = assistant.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM web_knowledge")
        knowledge_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM conversation_history")
        conversation_count = cursor.fetchone()[0]
        
        print(f"\nğŸ“š í˜„ì¬ ì§€ì‹ í˜„í™©:")
        print(f"   ìˆ˜ì§‘ëœ ì›¹ ì§€ì‹: {knowledge_count}ê°œ")
        print(f"   ëŒ€í™” ê¸°ë¡: {conversation_count}ê°œ")
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())